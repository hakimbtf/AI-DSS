# -*- coding: utf-8 -*-
"""IAIGHG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yyUSgHPkwYA6AY3-2QLDZk1-GFxxMP98
"""

import pandas as pd

# Loading the data
data = pd.read_csv('/content/drive/MyDrive/Msc Data Science BDDS/alu production data.csv', header=1)

data.head()

data.shape

data.info()

# Convert the date columns to datetime format
data['From'] = pd.to_datetime(data['From'])
data['To'] = pd.to_datetime(data['To'])

data.isnull().sum()

# Handle missing values. Here, I'm just filling them with zeros
data.fillna(0, inplace=True)

data.head()

data.isnull().sum()

# Remove columns with all zeros, if any
data = data.loc[:, (data != 0).any(axis=0)]

# Display cleaned data
data.head()

data.info()

# Drop unnecessary columns
data.drop(columns=[col for col in data.columns if 'Unnamed' in col], inplace=True)

data.info()

# Extract year from 'From' column
data['Year'] = data['From'].dt.year

# Group by year and Alumina Grade, then aggregate the sum
data = data.groupby(['Year', 'Alumina Grade']).sum(numeric_only=True).reset_index()

# Compute the number of days in each year
data['Days in Year'] = data['Year'].apply(lambda x: 366 if x%4==0 and (x%100!=0 or x%400==0) else 365)

# Compute the correct daily average
data['Corrected Daily Average'] = data['Total'] / data['Days in Year']

data.head()

# Drop unnecessary columns for clarity
data.drop(columns=['Days in Year', 'Daily Average'], inplace=True)

data.head()

data.rename(columns={'Corrected Daily Average': 'Daily Average'}, inplace=True)
data.head()

emissions_data=pd.read_csv('/content/drive/MyDrive/Msc Data Science BDDS/ghg data.csv')
emissions_data.head(11)

emissions_data.describe()

emissions_data.info()

data.shape

emissions_data.shape

# Convert the date columns to datetime format
emissions_data['From'] = pd.to_datetime(emissions_data['From'])
emissions_data['To'] = pd.to_datetime(emissions_data['To'])
emissions_data['Year'] = emissions_data['From'].dt.year

# Drop unnecessary columns
cols_to_drop = ['From', 'To', 'Unnamed: 2', 'Unnamed: 5', 'Unnamed: 7', 'Unnamed: 9', 'Unnamed: 11', 'Unnamed: 13', 'Unnamed: 15', 'Unnamed: 17', 'Unnamed: 19']
emissions_data.drop(columns=cols_to_drop, inplace=True)

emissions_data.shape

emissions_data.info()

emissions_data.head(100)

Year=emissions_data.pop('Year')
emissions_data.insert(0, 'Year', Year)
emissions_data.head(100)

emissions_data.info()

print(emissions_data['Unnamed: 3'].unique())

emissions_data.rename(columns={'Unnamed: 3': 'Emission Source'}, inplace=True)

emissions_data.info()

data.info()

# Filling NA values with 0
emissions_data.fillna(0, inplace=True)


emissions_data.info()

emissions_data.head(11)

data.info()
emissions_data.info()

# Merging the data
data = pd.merge(data, emissions_data, on='Year', how='inner')

data.info()

data.head(33)

data.shape

data.tail(33)

data.describe()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.lineplot(data, x='Year', y='Total', marker='o')
plt.title('Primary Aluminium Production Trend Over the Years')
plt.ylabel('Total Production in thousand metric tonnes of aluminium')
plt.xlabel('Year')
plt.grid(True)
plt.show()

regions = ['Africa & Asia (ex China)', 'North America', 'South America', 'Western & Central Europe', 'Russia & Eastern Europe', 'Oceania', 'China (Estimated)','Estimated Unreported to IAI']
for region in regions:
    plt.figure(figsize=(12, 6))
    sns.barplot(data, x='Year', y=region)
    plt.title(f'Yearly Production in {region}')
    plt.ylabel('Production Amount')
    plt.xlabel('Year')
    plt.show()

correlation_matrix = data.corr()
plt.figure(figsize=(16, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Between Primary ALuminium Production in Different Regions and GHG Emission Sources')
plt.show()

plt.figure(figsize=(12, 6))
sns.lineplot(data, x='Year', y='All', hue='Emission Source', marker='o')
plt.title('GHG Emission Trend Over the Years')
plt.ylabel('Total GHG Emissions')
plt.xlabel('Year')
plt.legend(title='Emission Source')
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 8))
sns.barplot(data, x='Year', y='All', hue='Emission Source')
plt.title('GHG Emissions by Source Over the Years')
plt.ylabel('GHG Emissions')
plt.xlabel('Year')
plt.legend(title='Emission Source', loc='upper left')
plt.show()

correlation_matrix = data[['Mining', 'Refining', 'Anode Production', 'Electrolysis', 'Casting', 'Recycling', 'Semis Production', 'Internal Scrap Remelting', 'All']].corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation between Regions and Emissions')
plt.show()

# List of regions from the production dataset
regions = ['Africa & Asia (ex China)', 'North America', 'South America', 'Western & Central Europe', 'Russia & Eastern Europe', 'Europe (inc Russia)', 'Oceania', 'China (Estimated)']

# List of production processes from the GHG dataset
processes = ['Mining', 'Refining', 'Anode Production', 'Electrolysis', 'Casting', 'Recycling', 'Semis Production', 'Internal Scrap Remelting']

correlation_matrix = data[regions + processes].corr()

# Extract the part of the correlation matrix that correlates regions with processes
region_process_correlation = correlation_matrix.loc[regions, processes]

plt.figure(figsize=(12, 8))
sns.heatmap(region_process_correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation between Aluminium Regional Production and Aluminium Production Processes')
plt.show()

"""We merged both production and emission data into a single dataframe named data with an Emission Source column.

The column Emission Source categorically represents the type of emissions for each entry. To analyze the correlation between regions and emission sources, we should consider the values in the columns for the production processes across different regions.

To achieve this, we'll pivot the dataframe such that the regions become columns, and the indices are the Emission Source and Year. This will give us the ability to compute correlation across regions for a given Emission Source.
"""

pivot_data = data.pivot_table(index=['Year', 'Emission Source'], values=regions, aggfunc='sum')
correlation_matrix = pivot_data.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation between Regions and Emission Sources')
plt.show()

data.to_csv('iaighg.csv', index=False)

data.to_csv('/content/drive/MyDrive/Msc Data Science BDDS/iaighg.csv')

filtered_data = data[['Year', 'Total', 'All']]
correlation_value = filtered_data['Total'].corr(filtered_data['All'])

plt.figure(figsize=(12, 6))

# Plotting Production values
plt.plot(filtered_data['Year'], filtered_data['Total'], label='Total Production', color='blue')

# Plotting GHG Emissions values on a secondary axis
ax2 = plt.gca().twinx()
ax2.plot(filtered_data['Year'], filtered_data['All'], label='Total Emissions', color='red')

plt.title('Trends of Aluminum Production vs GHG Emissions')
plt.show()

filtered_data.head(33)

data.dtypes

filtered_data = data[(data['Emission Source'] == 'Total-Cradle to Gate') & (data['Alumina Grade'] == 'Metallurgical')]
agg_data = filtered_data.groupby('Year').agg({'Total': 'sum', 'All': 'sum'}).reset_index()
correlation_value = agg_data['Total'].corr(agg_data['All'])
print(f"The correlation between Total Production and Total Emissions is: {correlation_value}")

plt.figure(figsize=(12, 6))

# Plotting Production values
plt.plot(agg_data['Year'], agg_data['Total'], label='Total Production', color='blue')

# Plotting GHG Emissions values on a secondary axis
ax2 = plt.gca().twinx()
ax2.plot(agg_data['Year'], agg_data['All'], label='Total Emissions', color='red')

plt.title('Trends of Aluminum Production vs GHG Emissions for Metallurgical Alumina and Total-Cradle to Gate Emissions')
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Plotting Production values
plt.plot(agg_data['Year'], agg_data['Total'], label='Total Production', color='blue', marker='o')

# Plotting GHG Emissions values on a secondary axis
ax2 = plt.gca().twinx()
ax2.plot(agg_data['Year'], agg_data['All'], label='Total Emissions', color='red', marker='o')

# Legends and Titles
plt.legend(loc="upper left")
ax2.legend(loc="upper right")
plt.title('Trends of Aluminum Production vs GHG Emissions for Metallurgical Alumina and Total-Cradle to Gate Emissions')
plt.grid(True, which='both', linestyle='--', linewidth=0.5)

# Show the plot
plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix with numeric_only=True
corr = filtered_data.corr(numeric_only=True)

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(14, 10))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

plt.title("Correlation Matrix Heatmap")
plt.show()

# Setting the aesthetics for the plots
sns.set_style('whitegrid')

# Plotting the distribution for 'Mining'
plt.figure(figsize=(12, 6))
sns.histplot(data['Mining'], kde=True)
plt.title('Distribution of Mining')
plt.show()

# Plotting the distribution for 'Refining'
plt.figure(figsize=(12, 6))
sns.histplot(data['Refining'], kde=True)
plt.title('Distribution of Refining')
plt.show()

# Boxplot for 'Mining'
plt.figure(figsize=(12, 6))
sns.boxplot(x=data['Mining'])
plt.title('Boxplot for Mining')
plt.show()

# Boxplot for 'Refining'
plt.figure(figsize=(12, 6))
sns.boxplot(x=data['Refining'])
plt.title('Boxplot for Refining')
plt.show()

# Boxplot for 'Anode Production'
plt.figure(figsize=(12, 6))
sns.boxplot(x=data['Anode Production'])
plt.title('Boxplot for Anode Production')
plt.show()

# Boxplot for 'Electrolysis'
plt.figure(figsize=(12, 6))
sns.boxplot(x=data['Electrolysis'])
plt.title('Boxplot for Electrolysis')
plt.show()

# Sum emissions for each process
process_sums = data.drop(columns=['Year', 'Emission Source']).sum()

# Exclude the problematic entries
filtered_processes = process_sums.drop(['Alumina Grade', 'Daily Average', 'All'])

# Sort the filtered data
sorted_processes = filtered_processes.sort_values(ascending=False)

print(sorted_processes)

print(process_sums)

from scipy.stats import pearsonr

coeff, p_value = pearsonr(data['Total'], data['All'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

# First, get the sum of emissions for all processes for each region
data['Total Emissions'] = data[['Electrolysis', 'Refining', 'Anode Production', 'Semis Production', 'Recycling', 'Internal Scrap Remelting', 'Casting', 'Mining']].sum(axis=1)

# Then, compute the correlation between 'Total' (production) and 'Total Emissions' (emissions)
correlation_coefficient = data['Total'].corr(data['Total Emissions'])

from scipy.stats import pearsonr

coeff, p_value = pearsonr(data['Total'], data['Total Emissions'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'China (Estimated)']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['China (Estimated)'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'Total']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['Total'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'Oceania']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['Oceania'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'Russia & Eastern Europe']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['Russia & Eastern Europe'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'North America']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['North America'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'Europe (inc Russia)']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['Europe (inc Russia)'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

filtered_data = data[data['Emission Source'] == 'Total-Cradle to Gate']
correlation_matrix = filtered_data.corr(numeric_only=True)
correlation_value = correlation_matrix.loc['All', 'South America']


# If you're using a library like scipy for statistical analysis
from scipy.stats import pearsonr

# Getting the p-value
corr_coefficient, p_value = pearsonr(filtered_data['All'], filtered_data['South America'])

# Check significance
if p_value < 0.05:
    print("The correlation is statistically significant with p-value:", p_value)
else:
    print("The correlation is NOT statistically significant with p-value:", p_value)

correlations_with_total = data.corr()['Total'].sort_values(ascending=False)
print(correlations_with_total)

correlations_with_All = data.corr()['All'].sort_values(ascending=False)
print(correlations_with_All)

data['Total_Growth'] = data['Total'].pct_change().fillna(0)

data.head(33)

regions = ['China (Estimated)', 'Oceania', 'South America', 'Africa & Asia (ex China)', 'Western & Central Europe', 'North America', 'Russia & Eastern Europe']
production = data[regions].sum()

plt.figure(figsize=(12, 6))
production.sort_values().plot(kind='barh', color='skyblue')
plt.title('Primary Aluminium Production by Region')
plt.xlabel('Production in thousand metric tonnes of aluminium')
plt.grid(axis='x')
plt.show()

plt.figure(figsize=(12, 6))
data[['Mining', 'Refining', 'Anode Production', 'Electrolysis', 'Casting']].boxplot()
plt.title('Distribution of GHG Emissions Across Processes')
plt.ylabel('GHG Emissions in million tonnes of CO2e')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

processes = ['Mining', 'Refining', 'Anode Production', 'Electrolysis', 'Casting', 'Recycling', 'Semis Production', 'Internal Scrap Remelting']
emissions_by_process = data[processes].mean()  # Using mean here as an example

plt.figure(figsize=(10, 7))
emissions_by_process.plot(kind='pie', autopct='%1.1f%%')
plt.title('GHG Emissions by Aluminium Production Process')
plt.ylabel('')
plt.show()

pip install tensorflow

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Drop non-numeric columns
data_clean = data.drop(columns=['Year', 'Emission Source', 'Alumina Grade'])

# Features and target
X = data_clean.drop(columns='Total')
y = data_clean['Total']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()

# Input layer
model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))

# Hidden layers
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))

# Output layer
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

loss = model.evaluate(X_test, y_test, verbose=0)
print(f'Mean Squared Error on Test Data: {loss}')

predictions = model.predict(X_test)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

plt.scatter(y_test, model.predict(X_test))
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Values")
plt.show()

residuals = y_test - model.predict(X_test).flatten()
plt.scatter(y_test, residuals)
plt.xlabel("Actual Values")
plt.ylabel("Residuals")
plt.title("Actual Values vs. Residuals")
plt.show()

import numpy as np
rmse = np.sqrt(33833788.0)
print("Root Mean Squared Error:", rmse)

from sklearn.metrics import mean_squared_error

baseline_predictions = np.full(y_test.shape, y_train.mean())
baseline_mse = mean_squared_error(y_test, baseline_predictions)
print("Baseline MSE:", baseline_mse)

import matplotlib.pyplot as plt

plt.scatter(y_test, model.predict(X_test))
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Values")
plt.show()

residuals = y_test - model.predict(X_test).flatten()
plt.scatter(y_test, residuals)
plt.xlabel("Actual Values")
plt.ylabel("Residuals")
plt.title("Actual Values vs. Residuals")
plt.show()

print("X_test shape:", X_test.shape)
print("Predicted emissions shape:", baseline_predictions.shape)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# standardizing the data:
scaler = StandardScaler().fit(X_train)
X_train_standardized = scaler.transform(X_train)
X_test_standardized = scaler.transform(X_test)

# Retraining the model:
model.fit(X_train_standardized, y_train, epochs=100, batch_size=16, verbose=1)

# Predict on X_test:
predicted_emissions = model.predict(X_test_standardized)

print("X_test shape:", X_test_standardized.shape)
print("Predicted emissions shape:", predicted_emissions.shape)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(y_test, color='blue', label='Actual Emissions')
plt.plot(predicted_emissions, color='red', linestyle='dashed', label='Predicted Emissions')
plt.title('Actual vs Predicted Emissions')
plt.xlabel('Sample Index')
plt.ylabel('Emissions')
plt.legend()
plt.show()

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Assuming 'data' is our DataFrame and it's indexed by year or some time-based index
# We want to predict 'Total' based on past values.

# Data Preparation: Transform DataFrame into appropriate shape for LSTM
values = data['Total'].values.reshape(-1, 1)  # Make it a column vector
X, y = [], []
for i in range(1, len(values)):
    X.append(values[i-1:i])  # Taking the previous year's value as input
    y.append(values[i])  # Current year's value as what we are trying to predict

X = np.array(X)
y = np.array(y)

# Splitting the data into training and testing sets
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Building the LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Training the model
model.fit(X_train, y_train, epochs=200, verbose=1)

# Making predictions
y_pred = model.predict(X_test)

# Compare 'y_pred' with 'y_test' to see how well the model is performing.

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')

import matplotlib.pyplot as plt

plt.figure(figsize=(14,7))
plt.plot(y_test, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.title('Actual vs. Predicted Values')
plt.show()

def forecast_future_values(model, initial_sequence, n_steps):
    '''Forecast future values using LSTM model

    Parameters:
    - model: trained LSTM model
    - initial_sequence: the last sequence of the training data
    - n_steps: number of future time steps to forecast

    Returns:
    - list of forecasted values
    '''

    sequence = list(initial_sequence)
    forecasted_values = []

    for _ in range(n_steps):
        # Prepare the sequence as input for the model
        input_sequence = np.array(sequence[-len(initial_sequence):]).reshape(1, len(initial_sequence), 1)

        # Predict the next value
        predicted_value = model.predict(input_sequence)[0,0]

        # Append the predicted value to sequence & forecasted_values list
        sequence.append(predicted_value)
        forecasted_values.append(predicted_value)

    return forecasted_values

# Taking the last sequence of training data as starting point
initial_sequence = X_train[-1, :, 0]

# Forecast next 10 values, for instance
forecasted_values = forecast_future_values(model, initial_sequence, 10)

print(forecasted_values)

# Fit a new scaler for the target data
y_scaler = StandardScaler()
y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))

# ... [train your LSTM here] ...

# Forecast future values using LSTM
forecasted_values = forecast_future_values(model, initial_sequence, 10)

# Inverse transform using the y_scaler
forecasted_values_original_scale = y_scaler.inverse_transform(np.array(forecasted_values).reshape(-1, 1))
print(forecasted_values_original_scale)

plt.figure(figsize=(15, 6))
plt.plot(data['Total'].values, label='Actual Values', color='blue')
plt.plot(range(len(data['Total']), len(data['Total']) + len(forecasted_values)), forecasted_values, color='red', label='Forecasted Values')
plt.title('LSTM Forecast vs. Actuals')
plt.legend()
plt.grid(True)
plt.show()

filtered_data = data[(data['Alumina Grade'] == 'Metallurgical') & (data['Emission Source'] == 'Total-Cradle to Gate')]
filtered_data.shape

data.shape

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Let's drop non-numeric columns and those which are not features
features = filtered_data.drop(columns=['Total', 'Alumina Grade', 'Emission Source'])

X = features.values
y = filtered_data['Total'].values

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Scaling the data
scaler_X = StandardScaler().fit(X_train)
X_train = scaler_X.transform(X_train)
X_test = scaler_X.transform(X_test)

scaler_y = StandardScaler().fit(y_train.reshape(-1, 1))
y_train = scaler_y.transform(y_train.reshape(-1, 1))
y_test = scaler_y.transform(y_test.reshape(-1, 1))

y_train.shape

X_train.shape

X_test.shape

y_test.shape

print(y_test)

print(X_test)

print(X_train)

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM

# Reshape input data to be suitable for LSTM layers (samples, timesteps, features)
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

# Building the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(LSTM(30, return_sequences=True))
model.add(LSTM(10))
model.add(Dense(1))

# Compiling the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Training the model
model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, shuffle=False, validation_data=(X_test, y_test))

predicted_values = model.predict(X_test)

# Inverse the scaling to get the original values
predicted_values_original_scale = scaler_y.inverse_transform(predicted_values)
y_test_original_scale = scaler_y.inverse_transform(y_test)

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test_original_scale, predicted_values_original_scale)
print(f'Mean Squared Error on Test Data: {mse}')

from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from keras.optimizers import Adam

# Assuming you've already loaded and processed the dataset...

# Define LSTM model
model = Sequential()

# Input layer
model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))  # Adjusted input shape

# Hidden layers
model.add(LSTM(50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=False))
model.add(Dropout(0.2))

# Output layer
model.add(Dense(1))

# Compile and train
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

# Evaluate
test_loss = model.evaluate(X_test, y_test)
print(f"Mean Squared Error on Test Data: {test_loss}")

# Predict on the test data
predicted_emissions = model.predict(X_test)

# Inverse transform the predictions and test data for visualization
y_test_original = scaler_y.inverse_transform(y_test)
predicted_emissions_original = scaler_y.inverse_transform(predicted_emissions)

# Plotting
plt.figure(figsize=(14,7))

# Plotting the actual values
plt.plot(y_test_original, label='True Values', color='blue')

# Plotting the predicted values
plt.plot(predicted_emissions_original, label='Predicted Values', color='red', linestyle='--')

plt.title('Comparison of Actual and Predicted Emissions')
plt.xlabel('Sample Index')
plt.ylabel('Emissions')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Features: just a series of increasing numbers for time steps
X = np.arange(len(data)).reshape(-1, 1)
y = data['All']  # Assuming emissions data is in the first column

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error on Test Data: {mse}")

!pip install pystan==2.19.1.1

!pip install fbprophet

pip install statsmodels

import numpy as np
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

df = pd.to_numeric(data['Total'], errors='coerce')
df['Total']= pd.to_numeric(data['Total'], errors='coerce')

print(data['Total'].unique())

size = int(len(df) * 0.8)
train, test = df['Total'][0:size], df['Total'][size:len(df)]

model = ARIMA(train, order=(5,1,0))
model_fit = model.fit()
print(model_fit.summary())

# Filtering data for "Metallurgical" under "Alumina Grade" with "Emission Source" of "Total-Cradle to Gate"
metallurgical_cradle_data = data[(data['Alumina Grade'] == 'Metallurgical') & (data['Emission Source'] == 'Total-Cradle to Gate')]

# Scatter plot with trend line focusing on "Metallurgical" under "Alumina Grade"
plt.figure(figsize=(12, 7))
plt.scatter(metallurgical_cradle_data['Total'], metallurgical_cradle_data['All'], color='green', label='Metallurgical Data points')

# Plotting a trend line for "Metallurgical" under "Alumina Grade"
if not metallurgical_cradle_data.empty:
    z_metallurgical_cradle = np.polyfit(metallurgical_cradle_data['Total'], metallurgical_cradle_data['All'], 1)
    p_metallurgical_cradle = np.poly1d(z_metallurgical_cradle)
    plt.plot(metallurgical_cradle_data['Total'], p_metallurgical_cradle(metallurgical_cradle_data['Total']), "g--", label='Metallurgical Trend line')

plt.title('Relationship between Total Production and Emissions for "Metallurgical" under "Alumina Grade"')
plt.xlabel('Total Production')
plt.ylabel('Emissions (All)')
plt.legend()

plt.grid(True)
plt.show()

# Filtering data for "Chemical" under "Alumina Grade" with "Emission Source" of "Total-Cradle to Gate"
chemical_cradle_data = data[(data['Alumina Grade'] == 'Chemical') & (data['Emission Source'] == 'Total-Cradle to Gate')]

# Scatter plot with trend line focusing on "Chemical" under "Alumina Grade"
plt.figure(figsize=(12, 7))
plt.scatter(chemical_cradle_data['Total'], chemical_cradle_data['All'], color='blue', label='Chemical Data points')

# Plotting a trend line for "Chemical" under "Alumina Grade"
if not chemical_cradle_data.empty:
    z_chemical_cradle = np.polyfit(chemical_cradle_data['Total'], chemical_cradle_data['All'], 1)
    p_chemical_cradle = np.poly1d(z_chemical_cradle)
    plt.plot(chemical_cradle_data['Total'], p_chemical_cradle(chemical_cradle_data['Total']), "b--", label='Chemical Trend line')

plt.title('Relationship between Total Production and Emissions for "Chemical" under "Alumina Grade"')
plt.xlabel('Total Production')
plt.ylabel('Emissions (All)')
plt.legend()

plt.grid(True)
plt.show()

# Scatter plot with trend line to trace the relationship between total production and emissions
plt.figure(figsize=(12, 7))
plt.scatter(data['Total'], data['All'], color='blue', label='Data points')
plt.title('Relationship between Total Production and Emissions')
plt.xlabel('Total Production')
plt.ylabel('Total Emissions (All)')

# Plotting a trend line
z = np.polyfit(data['Total'], data['All'], 1)
p = np.poly1d(z)
plt.plot(data['Total'], p(data['Total']), "r--", label='Trend line')
plt.legend()

plt.grid(True)
plt.show()

# Filtering data for "Total-Cradle to Gate" emissions
cradle_to_gate_data = data[data['Emission Source'] == 'Total-Cradle to Gate']

# Scatter plot with trend line focusing on "Cradle to Gate" emissions
plt.figure(figsize=(12, 7))
plt.scatter(cradle_to_gate_data['Total'], cradle_to_gate_data['All'], color='blue', label='Data points')
plt.title('Relationship between Total Production and "Total-Cradle to Gate" Emissions')
plt.xlabel('Total Production')
plt.ylabel('"Total-Cradle to Gate" Emissions')

# Plotting a trend line
z_cradle = np.polyfit(cradle_to_gate_data['Total'], cradle_to_gate_data['All'], 1)
p_cradle = np.poly1d(z_cradle)
plt.plot(cradle_to_gate_data['Total'], p_cradle(cradle_to_gate_data['Total']), "r--", label='Trend line')
plt.legend()

plt.grid(True)
plt.show()

# Filtering data for "Metallurgical" under "Alumina Grade" with "Emission Source" of "Total-Cradle to Gate"
metallurgical_cradle_data = data[(data['Alumina Grade'] == 'Metallurgical') & (data['Emission Source'] == 'Total-Cradle to Gate')]

# Scatter plot with trend line focusing on "Metallurgical" under "Alumina Grade"
plt.figure(figsize=(12, 7))
plt.scatter(metallurgical_cradle_data['Total'], metallurgical_cradle_data['All'], color='green', label='Metallurgical Data points')

# Plotting a trend line for "Metallurgical" under "Alumina Grade"
if not metallurgical_cradle_data.empty:
    z_metallurgical_cradle = np.polyfit(metallurgical_cradle_data['Total'], metallurgical_cradle_data['All'], 1)
    p_metallurgical_cradle = np.poly1d(z_metallurgical_cradle)
    plt.plot(metallurgical_cradle_data['Total'], p_metallurgical_cradle(metallurgical_cradle_data['Total']), "g--", label='Metallurgical Trend line')

plt.title('Relationship between Total Production and Emissions for "Metallurgical" under "Alumina Grade"')
plt.xlabel('Total Production')
plt.ylabel('Emissions (All)')
plt.legend()

plt.grid(True)
plt.show()

# Filtering data for "Chemical" under "Alumina Grade" with "Emission Source" of "Total-Cradle to Gate"
chemical_cradle_data = data[(data['Alumina Grade'] == 'Chemical') & (data['Emission Source'] == 'Total-Cradle to Gate')]

# Scatter plot with trend line focusing on "Chemical" under "Alumina Grade"
plt.figure(figsize=(12, 7))
plt.scatter(chemical_cradle_data['Total'], chemical_cradle_data['All'], color='blue', label='Chemical Data points')

# Plotting a trend line for "Chemical" under "Alumina Grade"
if not chemical_cradle_data.empty:
    z_chemical_cradle = np.polyfit(chemical_cradle_data['Total'], chemical_cradle_data['All'], 1)
    p_chemical_cradle = np.poly1d(z_chemical_cradle)
    plt.plot(chemical_cradle_data['Total'], p_chemical_cradle(chemical_cradle_data['Total']), "b--", label='Chemical Trend line')

plt.title('Relationship between Total Production and Emissions for "Chemical" under "Alumina Grade"')
plt.xlabel('Total Production')
plt.ylabel('Emissions (All)')
plt.legend()

plt.grid(True)
plt.show()

# Trend Analysis

plt.figure(figsize=(15, 10))

# Trend for "Chemical" under "Alumina Grade"
plt.subplot(2, 1, 1)
plt.plot(chemical_cradle_data['Year'], chemical_cradle_data['All'], label='Emissions (All)', marker='o', color='blue')
plt.plot(chemical_cradle_data['Year'], chemical_cradle_data['Total'], label='Total Production', marker='o', color='red')
plt.title('"Chemical" Process under "Alumina Grade" - Trend Analysis')
plt.xlabel('Year')
plt.ylabel('Value')
plt.legend()
plt.grid(True)

# Trend for "Metallurgical" under "Alumina Grade"
plt.subplot(2, 1, 2)
plt.plot(metallurgical_cradle_data['Year'], metallurgical_cradle_data['All'], label='Emissions (All)', marker='o', color='blue')
plt.plot(metallurgical_cradle_data['Year'], metallurgical_cradle_data['Total'], label='Total Production', marker='o', color='red')
plt.title('"Metallurgical" Process under "Alumina Grade" - Trend Analysis')
plt.xlabel('Year')
plt.ylabel('Value')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Correlation Analysis

# Calculating correlation for "Chemical" under "Alumina Grade"
correlation_chemical = chemical_cradle_data['Total'].corr(chemical_cradle_data['All'])

# Calculating correlation for "Metallurgical" under "Alumina Grade"
correlation_metallurgical = metallurgical_cradle_data['Total'].corr(metallurgical_cradle_data['All'])

correlation_chemical, correlation_metallurgical

# Descriptive Statistics

# Summary statistics for "Chemical" under "Alumina Grade"
desc_stats_chemical = chemical_cradle_data[['All', 'Total']].describe()

# Summary statistics for "Metallurgical" under "Alumina Grade"
desc_stats_metallurgical = metallurgical_cradle_data[['All', 'Total']].describe()

desc_stats_chemical, desc_stats_metallurgical

# Distribution Analysis

plt.figure(figsize=(15, 10))

# Histogram for "Chemical" under "Alumina Grade"
plt.subplot(2, 2, 1)
plt.hist(chemical_cradle_data['All'], bins=10, color='blue', alpha=0.7, label='Emissions (All)')
plt.hist(chemical_cradle_data['Total'], bins=10, color='red', alpha=0.5, label='Total Production')
plt.title('"Chemical" Process - Distribution Analysis')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()

# Histogram for "Metallurgical" under "Alumina Grade"
plt.subplot(2, 2, 2)
plt.hist(metallurgical_cradle_data['All'], bins=10, color='blue', alpha=0.7, label='Emissions (All)')
plt.hist(metallurgical_cradle_data['Total'], bins=10, color='red', alpha=0.5, label='Total Production')
plt.title('"Metallurgical" Process - Distribution Analysis')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.show()

# Function to forecast using ARIMA
def forecast_with_arima_corrected(data, column_name, steps=10):
    # Fit the ARIMA model
    model = ARIMA(data[column_name], order=(5,1,0))
    model_fit = model.fit()

    # Forecast
    forecast = model_fit.forecast(steps=steps)

    # Creating a future year array
    last_year = data['Year'].max()
    future_years = [year for year in range(last_year + 1, last_year + 1 + steps)]

    return future_years, forecast

# Forecasting for "Chemical" process under "Alumina Grade"
years_chemical_all, forecast_chemical_all = forecast_with_arima_corrected(chemical_cradle_data, 'All')
years_chemical_total, forecast_chemical_total = forecast_with_arima_corrected(chemical_cradle_data, 'Total')

years_chemical_all, forecast_chemical_all, years_chemical_total, forecast_chemical_total

# Forecasting for "Metallurgical" process under "Alumina Grade"
years_metallurgical_all, forecast_metallurgical_all = forecast_with_arima_corrected(metallurgical_cradle_data, 'All')
years_metallurgical_total, forecast_metallurgical_total = forecast_with_arima_corrected(metallurgical_cradle_data, 'Total')

years_metallurgical_all, forecast_metallurgical_all, years_metallurgical_total, forecast_metallurgical_total

# Correcting the function to access forecasted values
def apply_reduction_corrected(forecasted_values, reduction_percentage):
    reduced_values = [forecasted_values.iloc[0]]
    for i in range(1, len(forecasted_values)):
        reduced_value = reduced_values[i-1] * (1 - reduction_percentage/100)
        reduced_values.append(reduced_value)
    return reduced_values

# Applying the reduction scenarios to the forecasted emissions for "Chemical" process
moderate_reduction_chemical = apply_reduction_corrected(forecast_chemical_all, 5)
aggressive_reduction_chemical = apply_reduction_corrected(forecast_chemical_all, 10)

# Plotting the scenarios
plt.figure(figsize=(12, 7))
plt.plot(years_chemical_all, forecast_chemical_all, label='Forecasted Emissions', marker='o', color='blue')
plt.plot(years_chemical_all, moderate_reduction_chemical, label='Moderate Reduction (5%)', marker='o', color='green')
plt.plot(years_chemical_all, aggressive_reduction_chemical, label='Aggressive Reduction (10%)', marker='o', color='red')
plt.title('"Chemical" Process - Emission Reduction Scenarios')
plt.xlabel('Year')
plt.ylabel('Emissions (All)')
plt.legend()
plt.grid(True)
plt.show()

# Applying the reduction scenarios to the forecasted emissions for "Metallurgical" process
moderate_reduction_metallurgical = apply_reduction_corrected(forecast_metallurgical_all, 5)
aggressive_reduction_metallurgical = apply_reduction_corrected(forecast_metallurgical_all, 10)

# Plotting the scenarios for "Metallurgical" process
plt.figure(figsize=(12, 7))
plt.plot(years_metallurgical_all, forecast_metallurgical_all, label='Forecasted Emissions', marker='o', color='blue')
plt.plot(years_metallurgical_all, moderate_reduction_metallurgical, label='Moderate Reduction (5%)', marker='o', color='green')
plt.plot(years_metallurgical_all, aggressive_reduction_metallurgical, label='Aggressive Reduction (10%)', marker='o', color='red')
plt.title('"Metallurgical" Process - Emission Reduction Scenarios')
plt.xlabel('Year')
plt.ylabel('Emissions (All)')
plt.legend()
plt.grid(True)
plt.show()

# Plotting for "Chemical" process
plt.figure(figsize=(14, 8))

# Historical Data
plt.plot(chemical_cradle_data['Year'], chemical_cradle_data['All'], label='Historical Emissions', marker='o', color='blue', linestyle='dashed')
plt.plot(chemical_cradle_data['Year'], chemical_cradle_data['Total'], label='Historical Production', marker='o', color='red', linestyle='dashed')

# Forecasted Data without intervention
plt.plot(years_chemical_all, forecast_chemical_all, label='Forecasted Emissions', marker='o', color='blue')
plt.plot(years_chemical_total, forecast_chemical_total, label='Forecasted Production', marker='o', color='red')

# Emission Reduction Scenarios
plt.plot(years_chemical_all, moderate_reduction_chemical, label='Emissions with Moderate Reduction (5%)', marker='o', color='green')
plt.plot(years_chemical_all, aggressive_reduction_chemical, label='Emissions with Aggressive Reduction (10%)', marker='o', color='purple')

plt.title('"Chemical" Process - Historical, Forecasted, and Scenario Analysis')
plt.xlabel('Year')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plotting for "Metallurgical" process
plt.figure(figsize=(14, 8))

# Historical Data
plt.plot(metallurgical_cradle_data['Year'], metallurgical_cradle_data['All'], label='Historical Emissions', marker='o', color='blue', linestyle='dashed')
plt.plot(metallurgical_cradle_data['Year'], metallurgical_cradle_data['Total'], label='Historical Production', marker='o', color='red', linestyle='dashed')

# Forecasted Data without intervention
plt.plot(years_metallurgical_all, forecast_metallurgical_all, label='Forecasted Emissions', marker='o', color='blue')
plt.plot(years_metallurgical_total, forecast_metallurgical_total, label='Forecasted Production', marker='o', color='red')

# Emission Reduction Scenarios
plt.plot(years_metallurgical_all, moderate_reduction_metallurgical, label='Emissions with Moderate Reduction (5%)', marker='o', color='green')
plt.plot(years_metallurgical_all, aggressive_reduction_metallurgical, label='Emissions with Aggressive Reduction (10%)', marker='o', color='purple')

plt.title('"Metallurgical" Process - Historical, Forecasted, and Scenario Analysis')
plt.xlabel('Year')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Calculate the production-emission ratio for "Chemical" process
chemical_cradle_data['Ratio'] = chemical_cradle_data['Total'] / chemical_cradle_data['All']

# Calculate the forecasted ratio for "Chemical" process
forecasted_ratio_chemical = [prod / emit for prod, emit in zip(forecast_chemical_total, forecast_chemical_all)]

# Visualizing the Production-Emission Ratio for "Chemical" process
plt.figure(figsize=(12, 7))
plt.plot(chemical_cradle_data['Year'], chemical_cradle_data['Ratio'], label='Historical Ratio', marker='o', color='blue', linestyle='dashed')
plt.plot(years_chemical_all, forecasted_ratio_chemical, label='Forecasted Ratio', marker='o', color='green')
plt.title('"Chemical" Process - Production-Emission Ratio Analysis')
plt.xlabel('Year')
plt.ylabel('Production per unit of Emission (Ratio)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Calculate the emission per unit of aluminum for "Chemical" process
chemical_cradle_data['Emission_per_kg'] = chemical_cradle_data['All'] / chemical_cradle_data['Total']

# Calculate the forecasted emission per unit of aluminum for "Chemical" process
forecasted_emission_per_kg_chemical = [emit / prod for emit, prod in zip(forecast_chemical_all, forecast_chemical_total)]

# Visualizing the Emission per unit of Aluminum Production for "Chemical" process
plt.figure(figsize=(12, 7))
plt.plot(chemical_cradle_data['Year'], chemical_cradle_data['Emission_per_kg'], label='Historical Emission per kg', marker='o', color='blue', linestyle='dashed')
plt.plot(years_chemical_all, forecasted_emission_per_kg_chemical, label='Forecasted Emission per kg', marker='o', color='green')
plt.title('"Chemical" Process - Emission per kg of Aluminum Produced')
plt.xlabel('Year')
plt.ylabel('Emission per kg of Aluminum')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Calculate the emission per unit of aluminum for "Metallurgical" process
metallurgical_cradle_data['Emission_per_kg'] = metallurgical_cradle_data['All'] / metallurgical_cradle_data['Total']

# Calculate the forecasted emission per unit of aluminum for "Metallurgical" process
forecasted_emission_per_kg_metallurgical = [emit / prod for emit, prod in zip(forecast_metallurgical_all, forecast_metallurgical_total)]

# Visualizing the Emission per unit of Aluminum Production for "Metallurgical" process
plt.figure(figsize=(12, 7))
plt.plot(metallurgical_cradle_data['Year'], metallurgical_cradle_data['Emission_per_kg'], label='Historical Emission per kg', marker='o', color='blue', linestyle='dashed')
plt.plot(years_metallurgical_all, forecasted_emission_per_kg_metallurgical, label='Forecasted Emission per kg', marker='o', color='green')
plt.title('"Metallurgical" Process - Emission per kg of Aluminum Produced')
plt.xlabel('Year')
plt.ylabel('Emission per kg of Aluminum')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Emission intensities for "Metallurgical" process
metallurgical_intensities = list(metallurgical_cradle_data['Emission_per_kg']) + forecasted_emission_per_kg_metallurgical

# Emission intensities for "Chemical" process
chemical_intensities = list(chemical_cradle_data['Emission_per_kg']) + forecasted_emission_per_kg_chemical

# Years for plotting
years_combined = list(chemical_cradle_data['Year']) + years_chemical_all

# Visualizing the emission intensities for both processes
plt.figure(figsize=(14, 8))
plt.plot(years_combined, chemical_intensities, label='Chemical Process', marker='o', color='blue')
plt.plot(years_combined, metallurgical_intensities, label='Metallurgical Process', marker='o', color='green')
plt.title('Comparative Analysis of Emission Intensities')
plt.xlabel('Year')
plt.ylabel('Emission per kg of Aluminum')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Correcting the function to compute growth rate
def compute_growth_rate_corrected(values):
    values = values.reset_index(drop=True)
    growth_rates = [(values[i] - values[i-1]) / values[i-1] for i in range(1, len(values))]
    # Adding a NaN for the first value as there's no growth rate for the first data point
    growth_rates = [None] + growth_rates
    return growth_rates

# Calculate growth rates for "Chemical" process
chemical_emission_growth_rates = compute_growth_rate_corrected(chemical_cradle_data['All'])
chemical_production_growth_rates = compute_growth_rate_corrected(chemical_cradle_data['Total'])

# Calculate growth rates for "Metallurgical" process
metallurgical_emission_growth_rates = compute_growth_rate_corrected(metallurgical_cradle_data['All'])
metallurgical_production_growth_rates = compute_growth_rate_corrected(metallurgical_cradle_data['Total'])

chemical_emission_growth_rates, chemical_production_growth_rates, metallurgical_emission_growth_rates, metallurgical_production_growth_rates

# Visualizing the annual growth rates of emissions for both processes
plt.figure(figsize=(14, 8))

plt.plot(chemical_cradle_data['Year'], chemical_emission_growth_rates, label='Chemical Process Emissions', marker='o', color='blue', linestyle='dashed')
plt.plot(metallurgical_cradle_data['Year'], metallurgical_emission_growth_rates, label='Metallurgical Process Emissions', marker='o', color='green', linestyle='dashed')

plt.title('Comparative Analysis of Annual Emission Growth Rates')
plt.xlabel('Year')
plt.ylabel('Growth Rate')
plt.axhline(0, color='black',linewidth=0.5)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend()
plt.tight_layout()
plt.show()

# Visualizing the annual growth rates of production for both processes
plt.figure(figsize=(14, 8))

plt.plot(chemical_cradle_data['Year'], chemical_production_growth_rates, label='Chemical Process Production', marker='o', color='blue', linestyle='dashed')
plt.plot(metallurgical_cradle_data['Year'], metallurgical_production_growth_rates, label='Metallurgical Process Production', marker='o', color='green', linestyle='dashed')

plt.title('Comparative Analysis of Annual Production Growth Rates')
plt.xlabel('Year')
plt.ylabel('Growth Rate')
plt.axhline(0, color='black',linewidth=0.5)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend()
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Selecting relevant columns
data_modeling = cradle_to_gate_data[['Year', 'Total', 'All']]

# Splitting the data into training and testing sets (80% training, 20% testing)
train_data, test_data = train_test_split(data_modeling, test_size=0.2, random_state=42)

# Separating independent and dependent variables
X_train = train_data[['Year', 'Total']]
y_train = train_data['All']

X_test = test_data[['Year', 'Total']]
y_test = test_data['All']

# Initializing and training the linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predicting on the testing set
y_pred = linear_model.predict(X_test)

# Calculating the performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

mse, r2

from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

# Initializing models
ridge_model = Ridge()
random_forest_model = RandomForestRegressor()

# Using cross-validation to assess model performance
ridge_scores = cross_val_score(ridge_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
random_forest_scores = cross_val_score(random_forest_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Calculating average MSE for each model
ridge_avg_mse = -np.mean(ridge_scores)
random_forest_avg_mse = -np.mean(random_forest_scores)

ridge_avg_mse, random_forest_avg_mse

# Training the Ridge Regression model on the entire dataset
ridge_model.fit(data_modeling[['Year', 'Total']], data_modeling['All'])

# Hypothetical production values for the upcoming years (assuming a 5% annual increase)
upcoming_years = np.arange(2024, 2031, 1)
hypothetical_production = [data_modeling['Total'].iloc[-1] * (1.05 ** i) for i in range(1, 8)]

# Making predictions using the Ridge Regression model
predicted_emissions = ridge_model.predict(np.column_stack((upcoming_years, hypothetical_production)))

# Combining results
future_predictions = pd.DataFrame({
    'Year': upcoming_years,
    'Hypothetical Production': hypothetical_production,
    'Predicted Emissions': predicted_emissions
})

future_predictions

# Extracting the years and hypothetical production values for plotting
predicted_years = future_predictions['Year'].values
predicted_production = future_predictions['Hypothetical Production'].values
predicted_emission_values = future_predictions['Predicted Emissions'].values

# Plotting the original and predicted data
plt.figure(figsize=(12, 6))

# Plotting original data
plt.plot(data_modeling['Year'], data_modeling['All'], label='Actual Emissions', marker='o', color='blue')

# Plotting predicted data
plt.plot(predicted_years, predicted_emission_values, label='Predicted Emissions', marker='x', color='red', linestyle='--')

plt.title('Actual vs. Predicted Emissions using Ridge Regression')
plt.xlabel('Year')
plt.ylabel('Emissions (All)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge



# Filtering the data for the desired columns and rows
data_modeling = data[(data['Emission Source'] == 'Total-Cradle to Gate') & (data['Alumina Grade'] == 'Metallurgical')]
data_modeling = data_modeling[['Year', 'All', 'Total']]

# Preparing the data for Ridge Regression model
X = data_modeling[['Year', 'Total']]
y = data_modeling['All']

# Training the Ridge Regression model on the entire dataset
ridge_model = Ridge()
ridge_model.fit(X, y)

# Making predictions using the Ridge Regression model for the future scenarios
predicted_emissions = ridge_model.predict(np.column_stack((upcoming_years, hypothetical_production)))

# Plotting the original and predicted data
plt.figure(figsize=(12, 6))

# Plotting original data
plt.plot(data_modeling['Year'], data_modeling['All'], label='Actual Emissions', marker='o', color='blue')

# Plotting predicted data
plt.plot(predicted_years, predicted_emission_values, label='Predicted Emissions', marker='x', color='red', linestyle='--')

plt.title('Actual vs. Predicted Emissions using Ridge Regression')
plt.xlabel('Year')
plt.ylabel('Emissions (All)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Defining the upcoming years and hypothetical production values again
upcoming_years = np.array([2024, 2025, 2026, 2027, 2028, 2029, 2030])
last_known_production = data_modeling['Total'].values[-1]
growth_rate = 0.05
hypothetical_production = [last_known_production * ((1 + growth_rate) ** i) for i in range(1, 8)]

# Making predictions using the Ridge Regression model for the future scenarios
predicted_emission_values = ridge_model.predict(np.column_stack((upcoming_years, hypothetical_production)))

# Plotting the original and predicted data
plt.figure(figsize=(12, 6))

# Plotting original data
plt.plot(data_modeling['Year'], data_modeling['All'], label='Actual Emissions', marker='o', color='blue')

# Plotting predicted data
plt.plot(upcoming_years, predicted_emission_values, label='Predicted Emissions', marker='x', color='red', linestyle='--')

plt.title('Actual vs. Predicted Emissions using Ridge Regression')
plt.xlabel('Year')
plt.ylabel('Emissions (All)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Extending the upcoming years and hypothetical production values for the next 30 years
extended_years = np.array([i for i in range(2024, 2054)])
hypothetical_production_extended = [last_known_production * ((1 + growth_rate) ** i) for i in range(1, 31)]

# Making predictions using the Ridge Regression model for the extended scenarios
predicted_emission_extended = ridge_model.predict(np.column_stack((extended_years, hypothetical_production_extended)))

# Plotting the original and predicted data
plt.figure(figsize=(14, 7))

# Plotting original emissions data
plt.plot(data_modeling['Year'], data_modeling['All'], label='Actual Emissions', marker='o', color='blue')
# Plotting predicted emissions data
plt.plot(extended_years, predicted_emission_extended, label='Predicted Emissions', marker='x', color='red', linestyle='--')
# Plotting actual production data
plt.plot(data_modeling['Year'], data_modeling['Total'], label='Actual Production', marker='o', color='green')
# Plotting predicted production data
plt.plot(extended_years, hypothetical_production_extended, label='Predicted Production', marker='x', color='orange', linestyle='--')

plt.title('Actual vs. Predicted Emissions & Production for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Helper function to train and predict using Ridge Regression for both production and emissions
def ridge_regression_predictions(data_subset, years_to_predict, growth_rate=0.05):
    # Train Ridge Regression for production
    X_production = data_subset[['Year']]
    y_production = data_subset['Total']
    ridge_production = Ridge()
    ridge_production.fit(X_production, y_production)
    predicted_production = [data_subset['Total'].values[-1] * ((1 + growth_rate) ** i) for i in range(1, len(years_to_predict) + 1)]

    # Train Ridge Regression for emissions
    X_emissions = data_subset[['Year', 'Total']]
    y_emissions = data_subset['All']
    ridge_emissions = Ridge()
    ridge_emissions.fit(X_emissions, y_emissions)
    predicted_emissions = ridge_emissions.predict(np.column_stack((years_to_predict, predicted_production)))

    return predicted_production, predicted_emissions


# Predictions for Metallurgical
data_metallurgical = data[(data['Emission Source'] == 'Total-Cradle to Gate') & (data['Alumina Grade'] == 'Metallurgical')]
predicted_production_metallurgical, predicted_emissions_metallurgical = ridge_regression_predictions(data_metallurgical, extended_years)

# Predictions for Chemical
data_chemical = data[(data['Emission Source'] == 'Total-Cradle to Gate') & (data['Alumina Grade'] == 'Chemical')]
predicted_production_chemical, predicted_emissions_chemical = ridge_regression_predictions(data_chemical, extended_years)

# Plotting the data
fig, ax1 = plt.subplots(figsize=(14, 7))

# Metallurgical data
ax1.plot(data_metallurgical['Year'], data_metallurgical['All'], label='Actual Emissions (Metallurgical)', color='blue')
ax1.plot(extended_years, predicted_emissions_metallurgical, label='Predicted Emissions (Metallurgical)', linestyle='--', color='blue')
ax1.plot(data_metallurgical['Year'], data_metallurgical['Total'], label='Actual Production (Metallurgical)', color='green')
ax1.plot(extended_years, predicted_production_metallurgical, label='Predicted Production (Metallurgical)', linestyle='--', color='green')

# Chemical data
ax1.plot(data_chemical['Year'], data_chemical['All'], label='Actual Emissions (Chemical)', color='red')
ax1.plot(extended_years, predicted_emissions_chemical, label='Predicted Emissions (Chemical)', linestyle='--', color='red')
ax1.plot(data_chemical['Year'], data_chemical['Total'], label='Actual Production (Chemical)', color='orange')
ax1.plot(extended_years, predicted_production_chemical, label='Predicted Production (Chemical)', linestyle='--', color='orange')

# Setting labels, title, and legend
ax1.set_xlabel('Year')
ax1.set_ylabel('Emissions', color='black')
ax1.tick_params(axis='y', labelcolor='black')
ax1.set_title('Actual vs. Predicted Emissions & Production for Metallurgical and Chemical Grades')
ax1.legend(loc='upper left')

# Create a second y-axis for production values
ax2 = ax1.twinx()
ax2.set_ylabel('Production', color='black')
ax2.tick_params(axis='y', labelcolor='black')

plt.grid(True)
plt.tight_layout()
plt.show()

# Plotting the data separately for Metallurgical and Chemical

# Metallurgical
fig, ax1 = plt.subplots(figsize=(14, 7))
ax1.plot(data_metallurgical['Year'], data_metallurgical['All'], label='Actual Emissions (Metallurgical)', color='blue')
ax1.plot(extended_years, predicted_emissions_metallurgical, label='Predicted Emissions (Metallurgical)', linestyle='--', color='blue')
ax1.plot(data_metallurgical['Year'], data_metallurgical['Total'], label='Actual Production (Metallurgical)', color='green')
ax1.plot(extended_years, predicted_production_metallurgical, label='Predicted Production (Metallurgical)', linestyle='--', color='green')
ax1.set_xlabel('Year')
ax1.set_ylabel('Emissions', color='black')
ax1.tick_params(axis='y', labelcolor='black')
ax1.set_title('Actual vs. Predicted Emissions & Production for Metallurgical Grade')
ax1.legend(loc='upper left')
ax2 = ax1.twinx()
ax2.set_ylabel('Production', color='black')
ax2.tick_params(axis='y', labelcolor='black')
plt.grid(True)
plt.tight_layout()
plt.show()

# Chemical
fig, ax1 = plt.subplots(figsize=(14, 7))
ax1.plot(data_chemical['Year'], data_chemical['All'], label='Actual Emissions (Chemical)', color='red')
ax1.plot(extended_years, predicted_emissions_chemical, label='Predicted Emissions (Chemical)', linestyle='--', color='red')
ax1.plot(data_chemical['Year'], data_chemical['Total'], label='Actual Production (Chemical)', color='orange')
ax1.plot(extended_years, predicted_production_chemical, label='Predicted Production (Chemical)', linestyle='--', color='orange')
ax1.set_xlabel('Year')
ax1.set_ylabel('Emissions', color='black')
ax1.tick_params(axis='y', labelcolor='black')
ax1.set_title('Actual vs. Predicted Emissions & Production for Chemical Grade')
ax1.legend(loc='upper left')
ax2 = ax1.twinx()
ax2.set_ylabel('Production', color='black')
ax2.tick_params(axis='y', labelcolor='black')
plt.grid(True)
plt.tight_layout()
plt.show()

# Adjusting the plots to include the legend for emissions

# Metallurgical
fig, ax1 = plt.subplots(figsize=(14, 7))
ax1.plot(data_metallurgical['Year'], data_metallurgical['Total'], label='Actual Production (Metallurgical)', color='green')
ax1.plot(extended_years, predicted_production_metallurgical, label='Predicted Production (Metallurgical)', linestyle='--', color='green')
ax1.set_xlabel('Year')
ax1.set_ylabel('Production', color='black')
ax1.tick_params(axis='y', labelcolor='black')
ax1.set_title('Actual vs. Predicted Production & Emissions for Metallurgical Grade')

ax2 = ax1.twinx()
ax2.plot(data_metallurgical['Year'], data_metallurgical['All'], label='Actual Emissions (Metallurgical)', color='blue')
ax2.plot(extended_years, predicted_emissions_metallurgical, label='Predicted Emissions (Metallurgical)', linestyle='--', color='blue')
ax2.set_ylabel('Emissions', color='black')
ax2.tick_params(axis='y', labelcolor='black')

# Merging the legends from both axes
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc='upper left')

plt.grid(True)
plt.tight_layout()
plt.show()

# Chemical
fig, ax1 = plt.subplots(figsize=(14, 7))
ax1.plot(data_chemical['Year'], data_chemical['Total'], label='Actual Production (Chemical)', color='orange')
ax1.plot(extended_years, predicted_production_chemical, label='Predicted Production (Chemical)', linestyle='--', color='orange')
ax1.set_xlabel('Year')
ax1.set_ylabel('Production', color='black')
ax1.tick_params(axis='y', labelcolor='black')
ax1.set_title('Actual vs. Predicted Production & Emissions for Chemical Grade')

ax2 = ax1.twinx()
ax2.plot(data_chemical['Year'], data_chemical['All'], label='Actual Emissions (Chemical)', color='red')
ax2.plot(extended_years, predicted_emissions_chemical, label='Predicted Emissions (Chemical)', linestyle='--', color='red')
ax2.set_ylabel('Emissions', color='black')
ax2.tick_params(axis='y', labelcolor='black')

# Merging the legends from both axes
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc='upper left')

plt.grid(True)
plt.tight_layout()
plt.show()

# Splitting the data into training and testing sets
train_data = data_modeling[data_modeling['Year'] <= 2018]
test_data = data_modeling[data_modeling['Year'] > 2018]

# Training data for production and emissions
X_train_prod = train_data[['Year']]
y_train_prod = train_data['Total']
X_train_emissions = train_data[['Year', 'Total']]
y_train_emissions = train_data['All']

# Test data for production and emissions
X_test_prod = test_data[['Year']]
y_test_prod = test_data['Total']
X_test_emissions = test_data[['Year', 'Total']]
y_test_emissions = test_data['All']

# Training Ridge Regression models on training data
ridge_model_prod = Ridge()
ridge_model_prod.fit(X_train_prod, y_train_prod)
ridge_model_emissions = Ridge()
ridge_model_emissions.fit(X_train_emissions, y_train_emissions)

# Making predictions using the trained models on test data
predicted_prod = ridge_model_prod.predict(X_test_prod)
predicted_emissions = ridge_model_emissions.predict(X_test_emissions)

# Calculating the mean squared error (MSE) for production and emissions
from sklearn.metrics import mean_squared_error
mse_prod = mean_squared_error(y_test_prod, predicted_prod)
mse_emissions = mean_squared_error(y_test_emissions, predicted_emissions)

mse_prod, mse_emissions

# Plotting Actual vs Predicted values for both Production and Emissions

# For Production
plt.figure(figsize=(14, 7))
plt.plot(test_data['Year'], y_test_prod, label='Actual Production', color='green', marker='o')
plt.plot(test_data['Year'], predicted_prod, label='Predicted Production', color='green', linestyle='--', marker='x')
plt.title('Actual vs Predicted Production (Metallurgical)')
plt.xlabel('Year')
plt.ylabel('Production')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# For Emissions
plt.figure(figsize=(14, 7))
plt.plot(test_data['Year'], y_test_emissions, label='Actual Emissions', color='blue', marker='o')
plt.plot(test_data['Year'], predicted_emissions, label='Predicted Emissions', color='blue', linestyle='--', marker='x')
plt.title('Actual vs Predicted Emissions (Metallurgical)')
plt.xlabel('Year')
plt.ylabel('Emissions')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Making predictions on the training data
predicted_prod_train = ridge_model_prod.predict(X_train_prod)
predicted_emissions_train = ridge_model_emissions.predict(X_train_emissions)

# Calculating the mean squared error (MSE) for production and emissions on training data
mse_prod_train = mean_squared_error(y_train_prod, predicted_prod_train)
mse_emissions_train = mean_squared_error(y_train_emissions, predicted_emissions_train)

mse_prod_train, mse_emissions_train

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Initializing the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Training the model for production prediction
rf_model.fit(X_train_prod, y_train_prod)

# Predicting on test data
rf_predicted_prod = rf_model.predict(X_test_prod)

# Calculating the mean squared error (MSE) for production predictions using Random Forest
rf_mse_prod = mean_squared_error(y_test_prod, rf_predicted_prod)

rf_mse_prod

from sklearn.linear_model import Lasso, ElasticNet
from sklearn.ensemble import GradientBoostingRegressor

# Lasso Regression
lasso_model = Lasso()
lasso_model.fit(X_train_prod, y_train_prod)
lasso_predicted_prod = lasso_model.predict(X_test_prod)
lasso_mse_prod = mean_squared_error(y_test_prod, lasso_predicted_prod)

# Elastic Net Regression
elastic_net_model = ElasticNet()
elastic_net_model.fit(X_train_prod, y_train_prod)
elastic_net_predicted_prod = elastic_net_model.predict(X_test_prod)
elastic_net_mse_prod = mean_squared_error(y_test_prod, elastic_net_predicted_prod)

# Gradient Boosted Trees
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train_prod, y_train_prod)
gb_predicted_prod = gb_model.predict(X_test_prod)
gb_mse_prod = mean_squared_error(y_test_prod, gb_predicted_prod)

lasso_mse_prod, elastic_net_mse_prod, gb_mse_prod

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.metrics import mean_squared_error


# Filtering the data for Metallurgical grade and 'Total-Cradle to Gate' emission source
data_metallurgical = data[(data['Alumina Grade'] == 'Metallurgical') & (data['Emission Source'] == 'Total-Cradle to Gate')]

# Splitting the data into training and testing sets
train_data = data_metallurgical[data_metallurgical['Year'] <= 2018]
test_data = data_metallurgical[data_metallurgical['Year'] > 2018]

# Training data for production
X_train_prod = train_data[['Year']]
y_train_prod = train_data['Total']

# Test data for production
X_test_prod = test_data[['Year']]
y_test_prod = test_data['Total']

# Training the XGBoost model for production prediction
xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train_prod, y_train_prod)

# Predicting on test data
xgb_predicted_prod = xgb_model.predict(X_test_prod)

# Plotting the Actual vs Predicted values for Production using XGBoost
plt.figure(figsize=(14, 7))
plt.plot(test_data['Year'], y_test_prod, label='Actual Production', color='green', marker='o')
plt.plot(test_data['Year'], xgb_predicted_prod, label='Predicted Production (XGBoost)', color='green', linestyle='--', marker='x')
plt.title('Actual vs Predicted Production (Metallurgical) using XGBoost')
plt.xlabel('Year')
plt.ylabel('Production')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error



# Filtering the data for Metallurgical grade and 'Total-Cradle to Gate' emission source
data_metallurgical = data[(data['Alumina Grade'] == 'Metallurgical') & (data['Emission Source'] == 'Total-Cradle to Gate')]

# Splitting the data into training and testing sets
train_data = data_metallurgical[data_metallurgical['Year'] <= 2018]
test_data = data_metallurgical[data_metallurgical['Year'] > 2018]

# Training data for production and emissions
X_train = train_data[['Year']]
y_train_prod = train_data['Total']
y_train_emissions = train_data['All']

# Test data for production and emissions
X_test = test_data[['Year']]
y_test_prod = test_data['Total']
y_test_emissions = test_data['All']

# Retraining the XGBoost model for production prediction
xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=50, max_depth=3, random_state=42)
xgb_model.fit(X_train, y_train_prod)

# Retraining the Ridge Regression model for emission prediction
degree = 3
alpha = 1e-3
ridge_model_emissions = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))
ridge_model_emissions.fit(X_train, y_train_emissions)

# Predictions for production and emissions from 2005 onwards
xgb_prod_predictions = xgb_model.predict(X_test)
ridge_emissions_predictions = ridge_model_emissions.predict(X_test)

# Filtering the test data to include only the years from 2005 onwards
filtered_test_data = test_data[test_data['Year'] >= 2005]

# Creating a DataFrame to compare actual and predicted values for production and emissions from 2005 onwards
comparison_df = filtered_test_data[['Year', 'Total', 'All']].copy()
comparison_df['Predicted Production (XGBoost)'] = xgb_prod_predictions[:len(filtered_test_data)]
comparison_df['Predicted Emissions (Ridge)'] = ridge_emissions_predictions[:len(filtered_test_data)]

# Renaming columns for clarity
comparison_df.rename(columns={'Total': 'Actual Production', 'All': 'Actual Emissions'}, inplace=True)

comparison_df

from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPRegressor

# Normalize the data
scaler_X = MinMaxScaler()
scaler_Y_prod = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

y_train_prod_scaled = scaler_Y_prod.fit_transform(y_train_prod.values.reshape(-1, 1))
y_test_prod_scaled = scaler_Y_prod.transform(y_test_prod.values.reshape(-1, 1))

# Define the neural network
nn_model_prod = MLPRegressor(hidden_layer_sizes=(50, 30, 10), activation='relu', max_iter=1000, random_state=42)

# Train the model on the training data
nn_model_prod.fit(X_train_scaled, y_train_prod_scaled.ravel())

# Predict on the test data
nn_predicted_prod_scaled = nn_model_prod.predict(X_test_scaled)
nn_predicted_prod = scaler_Y_prod.inverse_transform(nn_predicted_prod_scaled.reshape(-1, 1))

# Calculate MSE for the neural network model
mse_nn_prod = mean_squared_error(y_test_prod, nn_predicted_prod)

mse_nn_prod

# Defining a simpler neural network
nn_model_prod_simple = MLPRegressor(hidden_layer_sizes=(20, 10), activation='relu', max_iter=500, random_state=42)

# Training the simpler model on the training data
nn_model_prod_simple.fit(X_train_scaled, y_train_prod_scaled.ravel())

# Predicting on the test data
nn_predicted_prod_scaled_simple = nn_model_prod_simple.predict(X_test_scaled)
nn_predicted_prod_simple = scaler_Y_prod.inverse_transform(nn_predicted_prod_scaled_simple.reshape(-1, 1))

# Calculate MSE for the simpler neural network model
mse_nn_prod_simple = mean_squared_error(y_test_prod, nn_predicted_prod_simple)

mse_nn_prod_simple

from keras.models import Sequential
from keras.layers import LSTM, Dense

# Reshaping data for LSTM (samples, time steps, features)
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Defining the LSTM model
lstm_model_prod = Sequential()
lstm_model_prod.add(LSTM(50, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
lstm_model_prod.add(Dense(1))
lstm_model_prod.compile(optimizer='adam', loss='mse')

# Training the LSTM model on the training data
lstm_model_prod.fit(X_train_reshaped, y_train_prod_scaled, epochs=100, verbose=0)

# Predicting on the test data
lstm_predicted_prod_scaled = lstm_model_prod.predict(X_test_reshaped)
lstm_predicted_prod = scaler_Y_prod.inverse_transform(lstm_predicted_prod_scaled)

# Calculate MSE for the LSTM model
mse_lstm_prod = mean_squared_error(y_test_prod, lstm_predicted_prod)

mse_lstm_prod

# Normalize the emissions data
scaler_Y_emissions = MinMaxScaler()

y_train_emissions_scaled = scaler_Y_emissions.fit_transform(y_train_emissions.values.reshape(-1, 1))
y_test_emissions_scaled = scaler_Y_emissions.transform(y_test_emissions.values.reshape(-1, 1))

# Defining the LSTM model for emissions
lstm_model_emissions = Sequential()
lstm_model_emissions.add(LSTM(50, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
lstm_model_emissions.add(Dense(1))
lstm_model_emissions.compile(optimizer='adam', loss='mse')

# Training the LSTM model on the training emissions data
lstm_model_emissions.fit(X_train_reshaped, y_train_emissions_scaled, epochs=100, verbose=0)

# Predicting on the test emissions data
lstm_predicted_emissions_scaled = lstm_model_emissions.predict(X_test_reshaped)
lstm_predicted_emissions = scaler_Y_emissions.inverse_transform(lstm_predicted_emissions_scaled)

# Calculate MSE for the LSTM emissions model
mse_lstm_emissions = mean_squared_error(y_test_emissions, lstm_predicted_emissions)

mse_lstm_emissions

# Defining and training the MLP model for production
mlp_model_prod = MLPRegressor(hidden_layer_sizes=(50, 30, 10), activation='relu', max_iter=500, random_state=42)
mlp_model_prod.fit(X_train_scaled, y_train_prod_scaled.ravel())

# Predicting on the test production data
mlp_predicted_prod_scaled = mlp_model_prod.predict(X_test_scaled)
mlp_predicted_prod = scaler_Y_prod.inverse_transform(mlp_predicted_prod_scaled.reshape(-1, 1))

# Calculate MSE for the MLP production model
mse_mlp_prod = mean_squared_error(y_test_prod, mlp_predicted_prod)

# Defining and training the MLP model for emissions
mlp_model_emissions = MLPRegressor(hidden_layer_sizes=(50, 30, 10), activation='relu', max_iter=500, random_state=42)
mlp_model_emissions.fit(X_train_scaled, y_train_emissions_scaled.ravel())

# Predicting on the test emissions data
mlp_predicted_emissions_scaled = mlp_model_emissions.predict(X_test_scaled)
mlp_predicted_emissions = scaler_Y_emissions.inverse_transform(mlp_predicted_emissions_scaled.reshape(-1, 1))

# Calculate MSE for the MLP emissions model
mse_mlp_emissions = mean_squared_error(y_test_emissions, mlp_predicted_emissions)

mse_mlp_prod, mse_mlp_emissions

from sklearn.ensemble import RandomForestRegressor

# Defining and training the Random Forest model for production
rf_model_prod = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model_prod.fit(X_train, y_train_prod)

# Predicting on the test production data
rf_predicted_prod = rf_model_prod.predict(X_test)
mse_rf_prod = mean_squared_error(y_test_prod, rf_predicted_prod)

# Defining and training the Random Forest model for emissions
rf_model_emissions = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model_emissions.fit(X_train, y_train_emissions)

# Predicting on the test emissions data
rf_predicted_emissions = rf_model_emissions.predict(X_test)
mse_rf_emissions = mean_squared_error(y_test_emissions, rf_predicted_emissions)

mse_rf_prod, mse_rf_emissions

from sklearn.ensemble import GradientBoostingRegressor

# Defining and training the Gradient Boosting model for production
gbm_model_prod = GradientBoostingRegressor(n_estimators=100, random_state=42)
gbm_model_prod.fit(X_train, y_train_prod)

# Predicting on the test production data
gbm_predicted_prod = gbm_model_prod.predict(X_test)
mse_gbm_prod = mean_squared_error(y_test_prod, gbm_predicted_prod)

# Defining and training the Gradient Boosting model for emissions
gbm_model_emissions = GradientBoostingRegressor(n_estimators=100, random_state=42)
gbm_model_emissions.fit(X_train, y_train_emissions)

# Predicting on the test emissions data
gbm_predicted_emissions = gbm_model_emissions.predict(X_test)
mse_gbm_emissions = mean_squared_error(y_test_emissions, gbm_predicted_emissions)

mse_gbm_prod, mse_gbm_emissions

from sklearn.svm import SVR

# Defining and training the SVR model for production
svr_model_prod = SVR(kernel='linear', C=1.0)
svr_model_prod.fit(X_train_scaled, y_train_prod_scaled.ravel())

# Predicting on the test production data
svr_predicted_prod_scaled = svr_model_prod.predict(X_test_scaled)
svr_predicted_prod = scaler_Y_prod.inverse_transform(svr_predicted_prod_scaled.reshape(-1, 1))
mse_svr_prod = mean_squared_error(y_test_prod, svr_predicted_prod)

# Defining and training the SVR model for emissions
svr_model_emissions = SVR(kernel='linear', C=1.0)
svr_model_emissions.fit(X_train_scaled, y_train_emissions_scaled.ravel())

# Predicting on the test emissions data
svr_predicted_emissions_scaled = svr_model_emissions.predict(X_test_scaled)
svr_predicted_emissions = scaler_Y_emissions.inverse_transform(svr_predicted_emissions_scaled.reshape(-1, 1))
mse_svr_emissions = mean_squared_error(y_test_emissions, svr_predicted_emissions)

mse_svr_prod, mse_svr_emissions

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

df_metallurgical = data[(data['Alumina Grade'] == 'Metallurgical') & (data['Emission Source'] == 'Total-Cradle to Gate')]
X = df_metallurgical[['Year']]
y_prod = df_metallurgical['Total']
y_emissions = df_metallurgical['All']

X_train, X_test, y_train_prod, y_test_prod = train_test_split(X, y_prod, test_size=0.2, shuffle=False)
_, _, y_train_emissions, y_test_emissions = train_test_split(X, y_emissions, test_size=0.2, shuffle=False)

scaler_X = StandardScaler()
scaler_Y_prod = StandardScaler()
scaler_Y_emissions = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

y_train_prod_scaled = scaler_Y_prod.fit_transform(y_train_prod.values.reshape(-1, 1))
y_train_emissions_scaled = scaler_Y_emissions.fit_transform(y_train_emissions.values.reshape(-1, 1))

# For Production
svr_prod = SVR(kernel='linear').fit(X_train_scaled, y_train_prod_scaled.ravel())
mlp_prod = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=5000).fit(X_train_scaled, y_train_prod_scaled.ravel())

# For Emissions
svr_emissions = SVR(kernel='linear').fit(X_train_scaled, y_train_emissions_scaled.ravel())
mlp_emissions = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=5000).fit(X_train_scaled, y_train_emissions_scaled.ravel())

# Production
svr_pred_prod = svr_prod.predict(X_test_scaled)
mlp_pred_prod = mlp_prod.predict(X_test_scaled)
mse_svr_prod = mean_squared_error(y_test_prod, scaler_Y_prod.inverse_transform(svr_pred_prod.reshape(-1, 1)))
mse_mlp_prod = mean_squared_error(y_test_prod, scaler_Y_prod.inverse_transform(mlp_pred_prod.reshape(-1, 1)))

# Emissions
svr_pred_emissions = svr_emissions.predict(X_test_scaled)
mlp_pred_emissions = mlp_emissions.predict(X_test_scaled)
mse_svr_emissions = mean_squared_error(y_test_emissions, scaler_Y_emissions.inverse_transform(svr_pred_emissions.reshape(-1, 1)))
mse_mlp_emissions = mean_squared_error(y_test_emissions, scaler_Y_emissions.inverse_transform(mlp_pred_emissions.reshape(-1, 1)))

# Training the SVR models for Production and Emissions

# For Production
svr_prod_model = SVR(kernel='linear')
svr_prod_model.fit(X_train_scaled, y_train_prod_scaled.ravel())

# For Emissions
svr_emissions_model = SVR(kernel='linear')
svr_emissions_model.fit(X_train_scaled, y_train_emissions_scaled.ravel())

# Making predictions using the trained SVR models

# Predictions for production
svr_prod_predictions_scaled = svr_prod_model.predict(X_test_scaled)
svr_prod_predictions = scaler_Y_prod.inverse_transform(svr_prod_predictions_scaled.reshape(-1, 1))

# Predictions for emissions
svr_emissions_predictions_scaled = svr_emissions_model.predict(X_test_scaled)
svr_emissions_predictions = scaler_Y_emissions.inverse_transform(svr_emissions_predictions_scaled.reshape(-1, 1))

# Visualizing the results

# Production
plt.figure(figsize=(12, 6))
plt.plot(X_test['Year'], y_test_prod, label='Actual Production', color='black')
plt.plot(X_test['Year'], svr_prod_predictions, label='SVR Predicted Production', linestyle='--')
plt.title('Production: Actual vs. Predicted using SVR')
plt.legend()
plt.show()

# Emissions
plt.figure(figsize=(12, 6))
plt.plot(X_test['Year'], y_test_emissions, label='Actual Emissions', color='black')
plt.plot(X_test['Year'], svr_emissions_predictions, label='SVR Predicted Emissions', linestyle='--')
plt.title('Emissions: Actual vs. Predicted using SVR')
plt.legend()
plt.show()

# Training the MLPRegressor models for Production and Emissions

# For Production
mlp_prod_model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=5000)
mlp_prod_model.fit(X_train_scaled, y_train_prod_scaled.ravel())

# For Emissions
mlp_emissions_model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=5000)
mlp_emissions_model.fit(X_train_scaled, y_train_emissions_scaled.ravel())

# Making predictions using the trained MLP models

# Predictions for production
mlp_prod_predictions_scaled = mlp_prod_model.predict(X_test_scaled)
mlp_prod_predictions = scaler_Y_prod.inverse_transform(mlp_prod_predictions_scaled.reshape(-1, 1))

# Predictions for emissions
mlp_emissions_predictions_scaled = mlp_emissions_model.predict(X_test_scaled)
mlp_emissions_predictions = scaler_Y_emissions.inverse_transform(mlp_emissions_predictions_scaled.reshape(-1, 1))

# Visualizing the results

# Production
plt.figure(figsize=(12, 6))
plt.plot(X_test['Year'], y_test_prod, label='Actual Production', color='black')
plt.plot(X_test['Year'], mlp_prod_predictions, label='MLP Predicted Production', linestyle='--')
plt.title('Production: Actual vs. Predicted using MLPRegressor')
plt.legend()
plt.show()

# Emissions
plt.figure(figsize=(12, 6))
plt.plot(X_test['Year'], y_test_emissions, label='Actual Emissions', color='black')
plt.plot(X_test['Year'], mlp_emissions_predictions, label='MLP Predicted Emissions', linestyle='--')
plt.title('Emissions: Actual vs. Predicted using MLPRegressor')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
import numpy as np

# Calculate MSE
mse_svr_prod = mean_squared_error(y_test_prod, svr_prod_predictions)
mse_svr_emissions = mean_squared_error(y_test_emissions, svr_emissions_predictions)
mse_mlp_prod = mean_squared_error(y_test_prod, mlp_prod_predictions)
mse_mlp_emissions = mean_squared_error(y_test_emissions, mlp_emissions_predictions)

# Calculate MAPE
def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape_svr_prod = mape(y_test_prod.values, svr_prod_predictions.ravel())
mape_svr_emissions = mape(y_test_emissions.values, svr_emissions_predictions.ravel())
mape_mlp_prod = mape(y_test_prod.values, mlp_prod_predictions.ravel())
mape_mlp_emissions = mape(y_test_emissions.values, mlp_emissions_predictions.ravel())

print("MSE SVR Prod:", mse_svr_prod)
print("MSE SVR Emissions:", mse_svr_emissions)
print("MSE MLP Prod:", mse_mlp_prod)
print("MSE MLP Emissions:", mse_mlp_emissions)
print("MAPE SVR Prod:", mape_svr_prod)
print("MAPE SVR Emissions:", mape_svr_emissions)
print("MAPE MLP Prod:", mape_mlp_prod)
print("MAPE MLP Emissions:", mape_mlp_emissions)

"""Updated comparison table that includes the Mean Squared Error (MSE) of all the models we've tested for both production and emissions:

| Model                         | MSE Production       | MSE Emissions  |
|-------------------------------|----------------------|----------------|
| Linear Regression             | \(8.34 \times 10^7\) | 1619.60        |
| Ridge Regression              | \(8.31 \times 10^7\) | 1618.71        |
| XGBoost                       | \(2.91 \times 10^7\) | 4023.63        |
| LSTM                          | \(1.91 \times 10^9\) | 98046.90       |
| MLPRegressor                  | \(2.11 \times 10^7\) | 90.43          |
| ARIMA                         | \(2.89 \times 10^7\) | 1368.04        |
| Support Vector Regression     | \(1.55 \times 10^6\) | 666.92         |
| Gradient Boosting Regressor   | \(2.90 \times 10^7\) | 3933.14        |
| Random Forest Regressor       | \(3.29 \times 10^7\) | 5677.20        |

This comprehensive table provides a clear overview of the performance of each model on the dataset.
"""

# Filter the dataset based on the corrected criteria
filtered_data = data[(data['Emission Source'] == 'Total-Cradle to Gate') &
                     (data['Alumina Grade'] == 'Metallurgical')]

# Display the first few rows of the filtered dataset
filtered_data.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define features and targets
X = filtered_data[['Africa & Asia (ex China)', 'North America', 'South America',
                   'Western & Central Europe', 'Russia & Eastern Europe', 'Oceania',
                   'China (Estimated)', 'Estimated Unreported to IAI']]
y_production = filtered_data['Total']
y_emission = filtered_data['All']

# Split the data into training and test sets for both targets
X_train_prod, X_test_prod, y_train_prod, y_test_prod = train_test_split(X, y_production, test_size=0.2, random_state=42)
X_train_emission, X_test_emission, y_train_emission, y_test_emission = train_test_split(X, y_emission, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_prod = scaler.fit_transform(X_train_prod)
X_test_prod = scaler.transform(X_test_prod)
X_train_emission = scaler.fit_transform(X_train_emission)
X_test_emission = scaler.transform(X_test_emission)

X_train_prod.shape, X_train_emission.shape

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Train SVR for Production Total
svr_prod = SVR(kernel='linear')
svr_prod.fit(X_train_prod, y_train_prod)

# Predict on the test set
svr_prod_predictions = svr_prod.predict(X_test_prod)

# Calculate the mean squared error for the predictions
mse_svr_prod = mean_squared_error(y_test_prod, svr_prod_predictions)

mse_svr_prod

# Train SVR for Emissions All
svr_emission = SVR(kernel='linear')
svr_emission.fit(X_train_emission, y_train_emission)

# Predict on the test set
svr_emission_predictions = svr_emission.predict(X_test_emission)

# Calculate the mean squared error for the predictions
mse_svr_emission = mean_squared_error(y_test_emission, svr_emission_predictions)

mse_svr_emission

from sklearn.neural_network import MLPRegressor

# Train MLP for Production Total
mlp_prod = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000, random_state=42)
mlp_prod.fit(X_train_prod, y_train_prod)

# Predict on the test set
mlp_prod_predictions = mlp_prod.predict(X_test_prod)

# Calculate the mean squared error for the predictions
mse_mlp_prod = mean_squared_error(y_test_prod, mlp_prod_predictions)

mse_mlp_prod

# Train MLP for Emissions All
mlp_emission = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000, random_state=42)
mlp_emission.fit(X_train_emission, y_train_emission)

# Predict on the test set
mlp_emission_predictions = mlp_emission.predict(X_test_emission)

# Calculate the mean squared error for the predictions
mse_mlp_emission = mean_squared_error(y_test_emission, mlp_emission_predictions)

mse_mlp_emission

import matplotlib.pyplot as plt

# Plot the predictions for "Production Total"
plt.figure(figsize=(12, 6))

# Actual data
plt.plot(y_test_prod.values, label='Actual', color='black', marker='o')

# SVR predictions
plt.plot(svr_prod_predictions, label='SVR Predictions', color='blue', linestyle='dashed', marker='x')

# MLP predictions
plt.plot(mlp_prod_predictions, label='MLP Predictions', color='red', linestyle='dotted', marker='s')

plt.title('Actual vs. Predicted "Production Total"')
plt.xlabel('Test Sample Index')
plt.ylabel('Production Total')
plt.legend()
plt.grid(True)
plt.show()

# Plot the predictions for "Emissions All"
plt.figure(figsize=(12, 6))

# Actual data
plt.plot(y_test_emission.values, label='Actual', color='black', marker='o')

# SVR predictions
plt.plot(svr_emission_predictions, label='SVR Predictions', color='blue', linestyle='dashed', marker='x')

# MLP predictions
plt.plot(mlp_emission_predictions, label='MLP Predictions', color='red', linestyle='dotted', marker='s')

plt.title('Actual vs. Predicted "Emissions All"')
plt.xlabel('Test Sample Index')
plt.ylabel('Emissions All')
plt.legend()
plt.grid(True)
plt.show()

# Scale the entire dataset
X_scaled_prod = scaler.fit_transform(X)
X_scaled_emission = scaler.fit_transform(X)

# Retrain SVR and MLP models using the entire dataset for "Production Total"
svr_prod.fit(X_scaled_prod, y_production)
mlp_prod.fit(X_scaled_prod, y_production)

# Retrain SVR and MLP models using the entire dataset for "Emissions All"
svr_emission.fit(X_scaled_emission, y_emission)
mlp_emission.fit(X_scaled_emission, y_emission)

# Check if the models have been retrained successfully
"Models retrained successfully."

from sklearn.linear_model import LinearRegression
import numpy as np

# Initialize a dictionary to store extrapolated feature values
future_features = {feature: [] for feature in X.columns}

# Number of years of existing data
existing_years = data['Year'].unique().shape[0]

# Create an array representing the years of the dataset
years_array = np.arange(existing_years).reshape(-1, 1)

# For each feature, fit a linear regression model and predict values for the next 30 years
for feature in X.columns:
    reg = LinearRegression().fit(years_array, X[feature].values)
    future_years_array = np.arange(existing_years, existing_years + 30).reshape(-1, 1)
    future_features[feature] = reg.predict(future_years_array)

# Convert the extrapolated feature values to a DataFrame
future_features_df = pd.DataFrame(future_features)

future_features_df.head()

# Scale the extrapolated feature values
future_features_scaled_prod = scaler.transform(future_features_df)
future_features_scaled_emission = scaler.transform(future_features_df)

# Predict "Production Total" for the next 30 years using both models
svr_prod_future_predictions = svr_prod.predict(future_features_scaled_prod)
mlp_prod_future_predictions = mlp_prod.predict(future_features_scaled_prod)

# Predict "Emissions All" for the next 30 years using both models
svr_emission_future_predictions = svr_emission.predict(future_features_scaled_emission)
mlp_emission_future_predictions = mlp_emission.predict(future_features_scaled_emission)

svr_prod_future_predictions[:5], mlp_prod_future_predictions[:5]  # Displaying first 5 predicted values for verification

# Plotting the actual data and future predictions for "Production Total"
plt.figure(figsize=(15, 7))

# Plotting the actual data
years = data['Year'].unique()
plt.plot(years, y_production.values, label='Actual Data', color='black', marker='o')

# Plotting the future predictions
future_years = np.arange(years[-1] + 1, years[-1] + 31)
plt.plot(future_years, svr_prod_future_predictions, label='SVR Future Predictions', color='blue', linestyle='dashed', marker='x')
plt.plot(future_years, mlp_prod_future_predictions, label='MLP Future Predictions', color='red', linestyle='dotted', marker='s')

plt.title('Actual vs. Predicted "Production Total" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Production Total')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the actual data and future predictions for "Emissions All"
plt.figure(figsize=(15, 7))

# Plotting the actual data
plt.plot(years, y_emission.values, label='Actual Data', color='black', marker='o')

# Plotting the future predictions
plt.plot(future_years, svr_emission_future_predictions, label='SVR Future Predictions', color='blue', linestyle='dashed', marker='x')
plt.plot(future_years, mlp_emission_future_predictions, label='MLP Future Predictions', color='red', linestyle='dotted', marker='s')

plt.title('Actual vs. Predicted "Emissions All" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Emissions All')
plt.legend()
plt.grid(True)
plt.show()

def recursive_forecasting(model, initial_features, years=30):
    """
    Recursive forecasting using the given model.
    """
    predictions = []

    current_features = initial_features.copy().reshape(1, -1)

    for _ in range(years):
        # Predict the next year's value
        prediction = model.predict(current_features)
        predictions.append(prediction[0])

        # Shift the features and use the prediction as the input for the next year
        current_features = np.roll(current_features, shift=-1)
        current_features[0][-1] = prediction

    return predictions

# Starting with the last year's features
initial_features_prod = X_scaled_prod[-1]

# Recursive forecasting for "Production Total"
svr_prod_recursive_predictions = recursive_forecasting(svr_prod, initial_features_prod)
mlp_prod_recursive_predictions = recursive_forecasting(mlp_prod, initial_features_prod)

svr_prod_recursive_predictions[:5], mlp_prod_recursive_predictions[:5]  # Displaying first 5 predicted values for verification

# Starting with the last year's features for "Emissions All"
initial_features_emission = X_scaled_emission[-1]

# Recursive forecasting for "Emissions All"
svr_emission_recursive_predictions = recursive_forecasting(svr_emission, initial_features_emission)
mlp_emission_recursive_predictions = recursive_forecasting(mlp_emission, initial_features_emission)

svr_emission_recursive_predictions[:5], mlp_emission_recursive_predictions[:5]  # Displaying first 5 predicted values for verification

# Plotting the actual data and future predictions for "Production Total"
plt.figure(figsize=(15, 7))

# Plotting the actual data
plt.plot(years, y_production.values, label='Actual Data', color='black', marker='o')

# Plotting the recursive forecasted predictions
plt.plot(future_years, svr_prod_recursive_predictions, label='SVR Recursive Forecast', color='blue', linestyle='dashed', marker='x')
plt.plot(future_years, mlp_prod_recursive_predictions, label='MLP Recursive Forecast', color='red', linestyle='dotted', marker='s')

plt.title('Actual vs. Recursive Forecasted "Production Total" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Production Total')
plt.legend()
plt.grid(True)
plt.yscale("log")  # Using a logarithmic scale due to the large range of values
plt.show()

# Plotting the actual data and future predictions for "Emissions All"
plt.figure(figsize=(15, 7))

# Plotting the actual data
plt.plot(years, y_emission.values, label='Actual Data', color='black', marker='o')

# Plotting the recursive forecasted predictions
plt.plot(future_years, svr_emission_recursive_predictions, label='SVR Recursive Forecast', color='blue', linestyle='dashed', marker='x')
plt.plot(future_years, mlp_emission_recursive_predictions, label='MLP Recursive Forecast', color='red', linestyle='dotted', marker='s')

plt.title('Actual vs. Recursive Forecasted "Emissions All" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Emissions All')
plt.legend()
plt.grid(True)
plt.yscale("log")  # Using a logarithmic scale due to the large range of values
plt.show()

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Function to check stationarity using Augmented Dickey-Fuller test
def check_stationarity(timeseries):
    result = adfuller(timeseries)
    return result[1] <= 0.05  # Return True if p-value is <= 0.05, indicating stationarity

# Check stationarity of "Production Total"
is_stationary = check_stationarity(y_production)

is_stationary

# Fit ARIMA(1,1,1) model
arima_model = ARIMA(y_production, order=(1,1,1))
arima_result = arima_model.fit()

# Forecast the next 30 years
arima_forecast = arima_result.forecast(steps=30)

arima_forecast[:5]  # Displaying the first 5 forecasted values for verification

from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Normalize the data between 0 and 1
scaler_lstm = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler_lstm.fit_transform(y_production.values.reshape(-1, 1))

# Convert time series data into sequences for LSTM
def create_sequences(data, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i+sequence_length])
        y.append(data[i+sequence_length])
    return np.array(X), np.array(y)

sequence_length = 5  # Using the last 5 years to predict the next year
X_lstm, y_lstm = create_sequences(scaled_data, sequence_length)

# Reshape input to be [samples, time steps, features]
X_lstm = np.reshape(X_lstm, (X_lstm.shape[0], X_lstm.shape[1], 1))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(X_lstm.shape[1], X_lstm.shape[2]), return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

# Train the LSTM model
model.fit(X_lstm, y_lstm, epochs=200, batch_size=5, verbose=0)

# Check if the model has been trained successfully
"Model trained successfully."

def lstm_recursive_forecasting(model, initial_sequence, steps=30):
    """
    Recursive forecasting using the LSTM model.
    """
    predictions = []
    current_sequence = initial_sequence.copy()

    for _ in range(steps):
        # Predict the next value using current sequence
        predicted_value = model.predict(current_sequence.reshape(1, sequence_length, 1))
        predictions.append(predicted_value[0][0])

        # Update the sequence with the predicted value for the next iteration
        current_sequence = np.roll(current_sequence, shift=-1)
        current_sequence[-1] = predicted_value

    return predictions

# Forecast the next 30 years using the LSTM model
initial_sequence = scaled_data[-sequence_length:]
lstm_forecast_scaled = lstm_recursive_forecasting(model, initial_sequence)

# Inverse transform the predictions to get actual values
lstm_forecast = scaler_lstm.inverse_transform(np.array(lstm_forecast_scaled).reshape(-1, 1))

lstm_forecast[:5]  # Displaying the first 5 forecasted values for verification

def lstm_recursive_forecasting(model, initial_sequence, steps=30):
    """
    Recursive forecasting using the LSTM model.
    """
    predictions = []
    current_sequence = initial_sequence.copy()

    for _ in range(steps):
        predicted_value = model.predict(current_sequence.reshape(1, sequence_length, 1))
        predictions.append(predicted_value[0][0])

        # Update the sequence with the predicted value
        current_sequence = np.roll(current_sequence, shift=-1)
        current_sequence[-1] = predicted_value

    return predictions

# Use the last sequence from the normalized data as the initial input
initial_sequence = scaled_data[-sequence_length:]

# Forecast the next 30 years using the LSTM model
lstm_forecast_scaled = lstm_recursive_forecasting(model, initial_sequence)

# Inverse transform the predictions to bring them back to the original scale
lstm_forecast = scaler_lstm.inverse_transform(np.array(lstm_forecast_scaled).reshape(-1, 1))

current_year = 2021
for year, value in zip(range(current_year + 1, current_year + 31), lstm_forecast):
    print(f"Year: {year}, Production Total: {value[0]}")

# Plotting the actual data and future predictions for "Production Total"
plt.figure(figsize=(15, 7))

# Plotting the actual data
plt.plot(years, y_production.values, label='Actual Data', color='black', marker='o')

# Plotting the LSTM future predictions
plt.plot(future_years, lstm_forecast, label='LSTM Forecast', color='green', linestyle='dashed', marker='x')

plt.title('Actual vs. LSTM Forecasted "Production Total" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Production Total')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the actual data, ARIMA forecast, and LSTM forecast for "Production Total"
plt.figure(figsize=(15, 7))

# Plotting the actual data
plt.plot(years, y_production.values, label='Actual Data', color='black', marker='o')

# Plotting the ARIMA future predictions
plt.plot(future_years, arima_forecast, label='ARIMA Forecast', color='blue', linestyle='dashed', marker='x')

# Plotting the LSTM future predictions
plt.plot(future_years, lstm_forecast, label='LSTM Forecast', color='green', linestyle='dotted', marker='s')

plt.title('Actual vs. ARIMA vs. LSTM Forecasted "Production Total" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Production Total')
plt.legend()
plt.grid(True)
plt.show()

# Difference the series to make it stationary (since we've already determined it's non-stationary)
diff_emission = y_emission.diff().dropna()

# Plot ACF and PACF to determine ARIMA parameters
fig, ax = plt.subplots(1, 2, figsize=(14, 4))

# ACF
plot_acf(diff_emission, ax=ax[0], lags=7)
ax[0].set_title('Autocorrelation Function')

# PACF
plot_pacf(diff_emission, ax=ax[1], lags=7)
ax[1].set_title('Partial Autocorrelation Function')

plt.tight_layout()
plt.show()

# Fit ARIMA(1,1,1) model for "Emissions All"
arima_model_emission = ARIMA(y_emission, order=(1,1,1))
arima_result_emission = arima_model_emission.fit()

# Forecast the next 30 years
arima_forecast_emission = arima_result_emission.forecast(steps=30)

arima_forecast_emission[:5]  # Displaying the first 5 forecasted values for verification

from sklearn.preprocessing import MinMaxScaler

scaler_emission = MinMaxScaler(feature_range=(0, 1))
scaled_emission = scaler_emission.fit_transform(y_emission.values.reshape(-1, 1))

sequence_length = 5

X_lstm_emission, y_lstm_emission = create_sequences(scaled_emission, sequence_length)

from keras.models import Sequential
from keras.layers import LSTM, Dense

model_emission = Sequential()
model_emission.add(LSTM(50, input_shape=(sequence_length, 1), return_sequences=True))
model_emission.add(LSTM(50))
model_emission.add(Dense(1))

model_emission.compile(optimizer='adam', loss='mean_squared_error')

model_emission.fit(X_lstm_emission, y_lstm_emission, epochs=100, batch_size=5, verbose=1)

initial_sequence_emission = scaled_emission[-sequence_length:]
lstm_forecast_scaled_emission = lstm_recursive_forecasting(model_emission, initial_sequence_emission, steps=30)

lstm_forecast_emission = scaler_emission.inverse_transform(np.array(lstm_forecast_scaled_emission).reshape(-1, 1))

lstm_forecast_emission

# Plotting the actual data, ARIMA forecast, and LSTM forecast for "Emissions All"
plt.figure(figsize=(15, 7))

# Plotting the actual data
plt.plot(years, y_emission.values, label='Actual Data', color='black', marker='o')

# Plotting the ARIMA future predictions
plt.plot(future_years, arima_forecast_emission, label='ARIMA Forecast', color='blue', linestyle='dashed', marker='x')

# Plotting the LSTM future predictions
plt.plot(future_years, lstm_forecast_emission, label='LSTM Forecast', color='green', linestyle='dotted', marker='s')

plt.title('Actual vs. ARIMA vs. LSTM Forecasted "Emissions All" for the Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Emissions All')
plt.legend()
plt.grid(True)
plt.show()

# Using the appropriate columns to split the data
X_train, X_test, y_production_train, y_production_test = train_test_split(X, y_production, test_size=0.2, random_state=42)
_, _, y_emission_train, y_emission_test = train_test_split(X, y_emission, test_size=0.2, random_state=42)

# Retraining SVR and MLP models

# SVR for "Production Total"
svr_production = SVR().fit(X_train, y_production_train)
svr_production_predictions = svr_production.predict(X_test)
svr_mse_production = mean_squared_error(y_production_test, svr_production_predictions)

# SVR for "Emissions All"
svr_emission = SVR().fit(X_train, y_emission_train)
svr_emission_predictions = svr_emission.predict(X_test)
svr_mse_emission = mean_squared_error(y_emission_test, svr_emission_predictions)

# MLP for "Production Total"
mlp_production = MLPRegressor(random_state=42, max_iter=500).fit(X_train, y_production_train)
mlp_production_predictions = mlp_production.predict(X_test)
mlp_mse_production = mean_squared_error(y_production_test, mlp_production_predictions)

# MLP for "Emissions All"
mlp_emission = MLPRegressor(random_state=42, max_iter=500).fit(X_train, y_emission_train)
mlp_emission_predictions = mlp_emission.predict(X_test)
mlp_mse_emission = mean_squared_error(y_emission_test, mlp_emission_predictions)

(svr_mse_production, mlp_mse_production, svr_mse_emission, mlp_mse_emission)

# Correcting the error and calculating MSE for the available years in the dataset

# Calculate MSE for ARIMA model for "Production Total"
arima_mse_production = mean_squared_error(y_production.values, arima_forecast[:len(y_production)])

# Calculate MSE for ARIMA model for "Emissions All"
arima_mse_emission = mean_squared_error(y_emission.values, arima_forecast_emission[:len(y_emission)])

# Calculate MSE for LSTM model for "Production Total"
lstm_mse_production = mean_squared_error(y_production.values, lstm_forecast[:len(y_production)])

# Calculate MSE for LSTM model for "Emissions All"
lstm_mse_emission = mean_squared_error(y_emission.values, lstm_forecast_emission[:len(y_emission)])

(arima_mse_production, lstm_mse_production, arima_mse_emission, lstm_mse_emission)

# Consolidating all MSE values for comparison
mse_comparison_updated = {
    'Model': ['SVR', 'MLP', 'ARIMA', 'LSTM'],
    'Production Total MSE': [svr_mse_production, mlp_mse_production, arima_mse_production, lstm_mse_production],
    'Emissions All MSE': [svr_mse_emission, mlp_mse_emission, arima_mse_emission, lstm_mse_emission]
}

mse_comparison_df_updated = pd.DataFrame(mse_comparison_updated)
mse_comparison_df_updated

def mlp_recursive_forecasting(model, initial_features, steps=30):
    """
    Uses the given model to recursively forecast 'steps' into the future.

    Parameters:
    - model: Trained machine learning model for forecasting.
    - initial_features: The initial feature set to start the forecasting.
    - steps: Number of steps to forecast into the future.

    Returns:
    - A list of forecasted values.
    """
    current_features = initial_features.copy()
    forecasts = []

    for _ in range(steps):
        # Predict the next value
        predicted_value = model.predict(current_features.reshape(1, -1))[0]

        # Append the forecasted value to our forecasts list
        forecasts.append(predicted_value)

        # Prepare the features for the next prediction
        current_features = np.roll(current_features, shift=-1)
        current_features[-1] = predicted_value

    return forecasts

# Forecasting the next 30 years for "Production Total" using MLP
mlp_forecast_production = mlp_recursive_forecasting(mlp_production, X.iloc[-1].values)

# Forecasting the next 30 years for "Emissions All" using MLP
mlp_forecast_emission = mlp_recursive_forecasting(mlp_emission, X.iloc[-1].values)

mlp_forecast_production[:5], mlp_forecast_emission[:5]  # Displaying the first 5 forecasted values for verification

# Setting up the years for x-axis using the previously stored 'years' variable
years_forecast = np.arange(years[-1] + 1, years[-1] + 31)

# Plotting "Production Total"
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.plot(years, y_production, label='Actual', color='black')
plt.plot(years_forecast, mlp_forecast_production, label='MLP Forecast', color='blue', linestyle='dashed')
plt.title('Production Total Forecast using MLP')
plt.xlabel('Year')
plt.ylabel('Production Total')
plt.legend()

# Plotting "Emissions All"
plt.subplot(1, 2, 2)
plt.plot(years, y_emission, label='Actual', color='black')
plt.plot(years_forecast, mlp_forecast_emission, label='MLP Forecast', color='red', linestyle='dashed')
plt.title('Emissions All Forecast using MLP')
plt.xlabel('Year')
plt.ylabel('Emissions All')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import xgboost as xgb

# Splitting the dataset into training and testing sets
X = filtered_data['Year'].values.reshape(-1, 1)
y = filtered_data['All'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Applying the Ridge Regression model
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_scaled, y_train)
ridge_predictions = ridge.predict(X_test_scaled)

# Applying the XGBoost model
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)
xg_reg.fit(X_train_scaled, y_train)
xgb_predictions = xg_reg.predict(X_test_scaled)

ridge_predictions, xgb_predictions

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from statsmodels.tsa.arima.model import ARIMA

# Creating a list for future years
future_years = np.array(range(X[-1][0] + 1, X[-1][0] + 31)).reshape(-1, 1)

# Placeholder for forecasted values
forecasts = {}

# Placeholder for MSE values
mse_values = {}

# Training and predicting using Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
forecasts['Linear Regression'] = lin_reg.predict(future_years)
mse_values['Linear Regression'] = mean_squared_error(y, lin_reg.predict(X))

# Training and predicting using Ridge Regression
ridge_reg = Ridge()
ridge_reg.fit(X_train_scaled, y_train)
forecasts['Ridge Regression'] = ridge_reg.predict(scaler.transform(future_years))
mse_values['Ridge Regression'] = mean_squared_error(y_test, ridge.predict(X_test_scaled))

# Training and predicting using XGBoost
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                           max_depth = 5, alpha = 10, n_estimators = 100)
xg_reg.fit(X_train_scaled, y_train)
forecasts['XGBoost'] = xg_reg.predict(scaler.transform(future_years))
mse_values['XGBoost'] = mean_squared_error(y_test, xg_reg.predict(X_test_scaled))

forecasts, mse_values

# Training and predicting using MLPRegressor
mlp_reg = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)
mlp_reg.fit(X_train_scaled, y_train)
forecasts['MLPRegressor'] = mlp_reg.predict(scaler.transform(future_years))
mse_values['MLPRegressor'] = mean_squared_error(y_test, mlp_reg.predict(X_test_scaled))

# Training and predicting using Support Vector Regression
svr = SVR(kernel='linear')
svr.fit(X_train_scaled, y_train)
forecasts['Support Vector Regression'] = svr.predict(scaler.transform(future_years))
mse_values['Support Vector Regression'] = mean_squared_error(y_test, svr.predict(X_test_scaled))

# Training and predicting using Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
gbr.fit(X_train_scaled, y_train)
forecasts['Gradient Boosting Regressor'] = gbr.predict(scaler.transform(future_years))
mse_values['Gradient Boosting Regressor'] = mean_squared_error(y_test, gbr.predict(X_test_scaled))

# Training and predicting using Random Forest Regressor
rfr = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
rfr.fit(X_train_scaled, y_train)
forecasts['Random Forest Regressor'] = rfr.predict(scaler.transform(future_years))
mse_values['Random Forest Regressor'] = mean_squared_error(y_test, rfr.predict(X_test_scaled))

forecasts, mse_values

lstm_forecast

lstm_forecast_emission

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import xgboost as xgb

# Splitting the dataset into training and testing sets
X = filtered_data['Year'].values.reshape(-1, 1)
y = filtered_data['Total'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Applying the Ridge Regression model
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_scaled, y_train)
ridge_predictions = ridge.predict(X_test_scaled)

# Applying the XGBoost model
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)
xg_reg.fit(X_train_scaled, y_train)
xgb_predictions = xg_reg.predict(X_test_scaled)

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from statsmodels.tsa.arima.model import ARIMA

# Creating a list for future years
future_years = np.array(range(X[-1][0] + 1, X[-1][0] + 31)).reshape(-1, 1)

# Placeholder for forecasted values
forecasts = {}

# Placeholder for MSE values
mse_values = {}

# Training and predicting using Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
forecasts['Linear Regression'] = lin_reg.predict(future_years)
mse_values['Linear Regression'] = mean_squared_error(y, lin_reg.predict(X))

# Training and predicting using Ridge Regression
ridge_reg = Ridge()
ridge_reg.fit(X_train_scaled, y_train)
forecasts['Ridge Regression'] = ridge_reg.predict(scaler.transform(future_years))
mse_values['Ridge Regression'] = mean_squared_error(y_test, ridge.predict(X_test_scaled))

# Training and predicting using XGBoost
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                           max_depth = 5, alpha = 10, n_estimators = 100)
xg_reg.fit(X_train_scaled, y_train)
forecasts['XGBoost'] = xg_reg.predict(scaler.transform(future_years))
mse_values['XGBoost'] = mean_squared_error(y_test, xg_reg.predict(X_test_scaled))
# Training and predicting using MLPRegressor
mlp_reg = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)
mlp_reg.fit(X_train_scaled, y_train)
forecasts['MLPRegressor'] = mlp_reg.predict(scaler.transform(future_years))
mse_values['MLPRegressor'] = mean_squared_error(y_test, mlp_reg.predict(X_test_scaled))

# Training and predicting using Support Vector Regression
svr = SVR(kernel='linear')
svr.fit(X_train_scaled, y_train)
forecasts['Support Vector Regression'] = svr.predict(scaler.transform(future_years))
mse_values['Support Vector Regression'] = mean_squared_error(y_test, svr.predict(X_test_scaled))

# Training and predicting using Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
gbr.fit(X_train_scaled, y_train)
forecasts['Gradient Boosting Regressor'] = gbr.predict(scaler.transform(future_years))
mse_values['Gradient Boosting Regressor'] = mean_squared_error(y_test, gbr.predict(X_test_scaled))

# Training and predicting using Random Forest Regressor
rfr = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
rfr.fit(X_train_scaled, y_train)
forecasts['Random Forest Regressor'] = rfr.predict(scaler.transform(future_years))
mse_values['Random Forest Regressor'] = mean_squared_error(y_test, rfr.predict(X_test_scaled))

forecasts, mse_values

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA


X = filtered_data['Year'].values.reshape(-1, 1)
y = filtered_data['All'].values

# Scaling the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Future years
future_years = np.array(range(X[-1][0] + 1, X[-1][0] + 31)).reshape(-1, 1)

# Models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "XGBoost": xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                                max_depth=5, alpha=10, n_estimators=100),
    "MLPRegressor": MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000),
    "Support Vector Regression": SVR(kernel='linear'),
    "Gradient Boosting Regressor": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3),
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, max_depth=3)
}

forecasts = {"Year": future_years.squeeze()}

mse_values = {}

for name, model in models.items():
    model.fit(X_scaled, y)
    forecasts[name] = model.predict(scaler.transform(future_years))
    mse_values[name] = mean_squared_error(y, model.predict(X_scaled))

# LSTM model
X_reshaped = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_reshaped.shape[1], 1)))
model.add(LSTM(units=50))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_reshaped, y, epochs=100, batch_size=32)
future_reshaped = np.reshape(scaler.transform(future_years), (len(future_years), 1, 1))
forecasts['LSTM'] = model.predict(future_reshaped).squeeze()

# ARIMA model
arima_model = ARIMA(y, order=(5,1,0))
arima_model_fit = arima_model.fit()
arima_forecast = arima_model_fit.forecast(steps=30)[0]
forecasts['ARIMA'] = arima_forecast

# Adjusting the lengths to match
start = len(y) - len(arima_model_fit.fittedvalues)
mse_values['ARIMA'] = mean_squared_error(y[start:], arima_model_fit.fittedvalues)


# Saving results
forecast_df = pd.DataFrame(forecasts)
forecast_df.to_csv("forecasted_values.csv", index=False)

mse_df = pd.DataFrame(list(mse_values.items()), columns=["Model", "MSE"])
mse_df.to_csv("mse_values.csv", index=False)

X = filtered_data['Year'].values.reshape(-1, 1)
y = filtered_data['Total'].values

# Scaling the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Future years
future_years = np.array(range(X[-1][0] + 1, X[-1][0] + 31)).reshape(-1, 1)

# Models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "XGBoost": xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                                max_depth=5, alpha=10, n_estimators=100),
    "MLPRegressor": MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000),
    "Support Vector Regression": SVR(kernel='linear'),
    "Gradient Boosting Regressor": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3),
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, max_depth=3)
}

forecasts = {"Year": future_years.squeeze()}

mse_values = {}

for name, model in models.items():
    model.fit(X_scaled, y)
    forecasts[name] = model.predict(scaler.transform(future_years))
    mse_values[name] = mean_squared_error(y, model.predict(X_scaled))

# LSTM model
X_reshaped = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_reshaped.shape[1], 1)))
model.add(LSTM(units=50))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_reshaped, y, epochs=100, batch_size=32)
future_reshaped = np.reshape(scaler.transform(future_years), (len(future_years), 1, 1))
forecasts['LSTM'] = model.predict(future_reshaped).squeeze()

# ARIMA model
arima_model = ARIMA(y, order=(5,1,0))
arima_model_fit = arima_model.fit()
arima_forecast = arima_model_fit.forecast(steps=30)[0]
forecasts['ARIMA'] = arima_forecast

# Adjusting the lengths to match
start = len(y) - len(arima_model_fit.fittedvalues)
mse_values['ARIMA'] = mean_squared_error(y[start:], arima_model_fit.fittedvalues)


# Saving results
forecast_df = pd.DataFrame(forecasts)
forecast_df.to_csv("forecasted_productionvalues.csv", index=False)

mse_df = pd.DataFrame(list(mse_values.items()), columns=["Model", "MSE"])
mse_df.to_csv("mse_productionvalues.csv", index=False)

# Loading the provided forecasted values and MSE values
forecasted_values_df = pd.read_csv("forecasted_values.csv")
mse_values_df = pd.read_csv("mse_values.csv")

# Plotting the forecasted values
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))
for column in forecasted_values_df.columns[1:]:
    plt.plot(forecasted_values_df['Year'], forecasted_values_df[column], label=column)

plt.title('Forecasted Values for Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Forecasted Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plotting the MSE values
plt.figure(figsize=(15, 5))
plt.barh(mse_values_df['Model'], mse_values_df['MSE'], color='skyblue')
plt.xlabel('Mean Squared Error (MSE)')
plt.title('MSE Values for Different Models')
plt.tight_layout()
plt.show()

# Loading the provided forecasted values and MSE values
forecasted_values_df = pd.read_csv("forecasted_productionvalues.csv")
mse_values_df = pd.read_csv("mse_productionvalues.csv")

# Plotting the forecasted values
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))
for column in forecasted_values_df.columns[1:]:
    plt.plot(forecasted_values_df['Year'], forecasted_values_df[column], label=column)

plt.title('Forecasted Values for Next 30 Years')
plt.xlabel('Year')
plt.ylabel('Forecasted Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plotting the MSE values
plt.figure(figsize=(15, 5))
plt.barh(mse_values_df['Model'], mse_values_df['MSE'], color='skyblue')
plt.xlabel('Mean Squared Error (MSE)')
plt.title('MSE Values for Different Models')
plt.tight_layout()
plt.show()

# Loading the iaighg.csv file and applying the filters
iaighg_df = data
filtered_df = iaighg_df[(iaighg_df['Alumina Grade'] == 'Metallurgical') &
                        (iaighg_df['Emission Source'] == 'Total-Cradle to Gate')]

# Combining actual and forecasted data
combined_df = pd.concat([filtered_df, forecasted_values_df], ignore_index=True)

# Plotting the combined data
plt.figure(figsize=(15, 8))

# Plotting actual data
plt.plot(filtered_df['Year'], filtered_df['All'], label='Actual Data', color='black', linewidth=2)

# Plotting forecasted values
for column in forecasted_values_df.columns[1:]:
    plt.plot(combined_df['Year'], combined_df[column], label=column)

plt.title('Actual and Forecasted Values')
plt.xlabel('Year')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plotting the MSE values
plt.figure(figsize=(15, 5))
plt.barh(mse_values_df['Model'], mse_values_df['MSE'], color='skyblue')
plt.xlabel('Mean Squared Error (MSE)')
plt.title('MSE Values for Different Models')
plt.tight_layout()
plt.show()